{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = './data/IMDb/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (37500,)\n",
      "shape of test data is (12500,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['review'].values,df['sentiment'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usagers3/opmos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-86a4e789bb87>:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    37500.000000\n",
       "mean        69.235120\n",
       "std         48.089812\n",
       "min          2.000000\n",
       "25%         39.000000\n",
       "50%         54.000000\n",
       "75%         84.000000\n",
       "max        652.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have very less number of reviews with length > 500.\n",
    "#So we will consideronly those below it.\n",
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 100\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([100, 500])\n",
      "Sample input: \n",
      " tensor([[  0,   0,   0,  ...,  83,  32, 586],\n",
      "        [  0,   0,   0,  ...,  26,  13,  45],\n",
      "        [  0,   0,   0,  ...,  23, 133, 127],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 509, 854, 146],\n",
      "        [  0,   0,   0,  ..., 143,  25,  43],\n",
      "        [  0,   0,   0,  ...,  66,   3, 243]])\n",
      "Sample input: \n",
      " tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        #super(SentimentRNN,self).__init__()\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "model_name = 'lstm-imdb'\n",
    "training_round = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMRNN(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, epochs, optimizer,criterion, train_loader, valid_loader, type_, batch_size=100): \n",
    "    clip = 5\n",
    "    #epochs = 50 \n",
    "    valid_loss_min = np.Inf\n",
    "    # train for some number of epochs\n",
    "    epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "    epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    since = time.time()\n",
    "    \n",
    "    data_file = open('./data/26-10-2021/Train1/pytorch/lstm/torch_exp_{}_{}_{}.csv'.format(model_name, date, training_round), mode='w+', newline='', encoding='utf-8')\n",
    "    data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['Epoch', 'optimizer','Train_loss', 'Train_acc', \"val_loss\", \"Val_acc\", 'time','Elapse_time','date'])\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        since_1 = time.time()\n",
    "        train_losses = []\n",
    "        train_acc = 0.0\n",
    "        model.train()\n",
    "        # initialize hidden state \n",
    "        h = model.init_hidden(batch_size)\n",
    "        for inputs, labels in train_loader:\n",
    "        \n",
    "            inputs, labels = inputs.to(device), labels.to(device)   \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            #print(h)\n",
    "            if type_ == 'GRU':\n",
    "                h = h.data # GRU\n",
    "            else:\n",
    "                h = tuple([each.data for each in h]) # LSTM\n",
    "            \n",
    "            \n",
    "        \n",
    "            model.zero_grad()\n",
    "            output,h = model(inputs,h)\n",
    "        \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            # calculating accuracy\n",
    "            accuracy = acc(output,labels)\n",
    "            train_acc += accuracy\n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "        val_h = model.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        val_acc = 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valid_loader: \n",
    "            if type_ == 'GRU':\n",
    "                val_h = val_h.data # GRU\n",
    "            else:\n",
    "                val_h = tuple([each.data for each in val_h])# LSTM\n",
    "                #h = tuple([each.data for each in h]) # LSTM\n",
    "                \n",
    "            #val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "        epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "        epoch_tr_loss.append(epoch_train_loss)\n",
    "        epoch_vl_loss.append(epoch_val_loss)\n",
    "        epoch_tr_acc.append(epoch_train_acc)\n",
    "        epoch_vl_acc.append(epoch_val_acc)\n",
    "        print(f'Epoch {epoch+1}') \n",
    "        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "        print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "        #if epoch_val_loss <= valid_loss_min:\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), './data/26-10-2021/Train1/pytorch/{}/torch_state_{}_{}_{}.pb'.format(type_.lower(), model_name, date, training_round))\n",
    "            best_acc = epoch_val_acc\n",
    "            torch.save(model, './data/26-10-2021/Train1/pytorch/{}/torch_{}_{}_{}.pth'.format(type_.lower(),model_name, date, training_round))\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "            valid_loss_min = epoch_val_loss\n",
    "        time_elapsed_1 = time.time() - since_1\n",
    "        data_writer.writerow([ epoch+1, optimizer, epoch_train_loss, epoch_train_acc, epoch_val_loss, epoch_val_acc, time.time(),time_elapsed_1,date])\n",
    "\n",
    "        print(25*'==')\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    data_writer.writerow([ 'Best val Acc: {:4f}'.format(best_acc), time.time(),'Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60),''])\n",
    "    data_file.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.5177698908249537 val_loss : 0.423690080165863\n",
      "train_accuracy : 74.29333333333334 val_accuracy : 80.88\n",
      "Validation loss decreased (inf --> 0.423690).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.38540057973066966 val_loss : 0.3956813654899597\n",
      "train_accuracy : 83.456 val_accuracy : 83.56\n",
      "Validation loss decreased (0.423690 --> 0.395681).  Saving model ...\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.3443551641702652 val_loss : 0.3541071031093597\n",
      "train_accuracy : 85.504 val_accuracy : 84.408\n",
      "Validation loss decreased (0.395681 --> 0.354107).  Saving model ...\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.31061699426174166 val_loss : 0.34345653975009915\n",
      "train_accuracy : 87.24533333333333 val_accuracy : 85.152\n",
      "Validation loss decreased (0.354107 --> 0.343457).  Saving model ...\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.280968092083931 val_loss : 0.37279896187782285\n",
      "train_accuracy : 88.72266666666667 val_accuracy : 85.176\n",
      "Validation loss decreased (0.343457 --> 0.372799).  Saving model ...\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss : 0.2563993522326152 val_loss : 0.3679086592197418\n",
      "train_accuracy : 89.67466666666667 val_accuracy : 84.84\n",
      "==================================================\n",
      "Epoch 7\n",
      "train_loss : 0.21333499848842621 val_loss : 0.39944821095466615\n",
      "train_accuracy : 91.78666666666668 val_accuracy : 84.896\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss : 0.1630602574646473 val_loss : 0.4647751693725586\n",
      "train_accuracy : 93.856 val_accuracy : 83.968\n",
      "==================================================\n",
      "Epoch 9\n",
      "train_loss : 0.12055963969230651 val_loss : 0.5085231469869613\n",
      "train_accuracy : 95.47733333333333 val_accuracy : 83.696\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss : 0.08326431008676688 val_loss : 0.6202407656908036\n",
      "train_accuracy : 97.04 val_accuracy : 83.576\n",
      "==================================================\n",
      "Epoch 11\n",
      "train_loss : 0.053790603768080474 val_loss : 0.7171952958106995\n",
      "train_accuracy : 98.16 val_accuracy : 83.21600000000001\n",
      "==================================================\n",
      "Epoch 12\n",
      "train_loss : 0.035299579694246255 val_loss : 0.8410574033260345\n",
      "train_accuracy : 98.78133333333334 val_accuracy : 83.024\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#date = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "epochs = 50\n",
    "model_ft = train_rnn(model, epochs, optimizer,criterion, train_loader, valid_loader, 'LSTM')\n",
    "#torch.save(model, './data/23-08-2021/torch_{}_{}.pth'.format(model_name, date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2829279"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "since_0 = time.time()\n",
    "#model.load_state_dict(torch.load('./data/23-08-2021/torch_LSTM-IMDb_2021-09-08-20:38:47.pt'))\n",
    "#model = \n",
    "torch.load('./data/23-08-2021/torch_GRU-IMDb.pth')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize('./data/23-08-2021/torch_GRU-IMDb.pth')\n",
    "size0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since_0 = time.time()\n",
    "#model.load_state_dict(torch.load('./data/23-08-2021/torch_LSTM-IMDb_2021-09-08-20:38:47.pt'))\n",
    "#model = \n",
    "#model = torch.load('./data/23-08-2021/torch_LSTM-IMDb.pth')\n",
    "#t_elapsed_0 = time.time() - since_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, abs_errors,rel_errors, t0_list, t1_list, t2_list, t3_list, s_list):\n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "   \n",
    "    print(\"converting for batch: \", i)\n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "    #inputs, labels = x.to(device), labels.to(device)\n",
    "\n",
    "    output, val_h = model(x, val_h)\n",
    "    h0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    c0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    hidden = (h0,c0)\n",
    "\n",
    "    since_1 = time.time()\n",
    "    out, (hn, cn) = model(x, (h0, c0))\n",
    "    t_elapsed_1 = time.time() - since_1\n",
    "    # Export the model\n",
    "    since_1 = time.time()\n",
    "    \n",
    "    input_names = [\"input\", \"h0\", \"c0\"]\n",
    "    output_names = [\"output\", \"hn\", \"cn\"]\n",
    "\n",
    "    torch.onnx.export(model, (x, (h0, c0)), \n",
    "                  \"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name),\n",
    "                      input_names=input_names, output_names=output_names)\n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    onnx_model = onnx.load(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    size2 = os.path.getsize(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    s_list.append(size2)\n",
    "    \n",
    "    def to_numpy(tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    ort_session = onnxruntime.InferenceSession(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(x), 'h0':to_numpy(h0), 'c0':to_numpy(c0)})\n",
    "    #ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "    #ort_outs = ort_session.run(None, ort_inputs)\n",
    "        \n",
    "    t_elapsed_3 = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"*********\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    t0_list.append(t_elapsed_0)\n",
    "    t1_list.append(t_elapsed_1)\n",
    "    t2_list.append(t_elapsed_2)\n",
    "    t3_list.append(t_elapsed_3)\n",
    "    \n",
    "    abs_err = np.absolute(to_numpy(out)-ort_outs[0])\n",
    "    rel_err = np.absolute(to_numpy(out)-ort_outs[0])/ np.absolute(ort_outs[0])\n",
    "    abs_errors.append(abs_err)\n",
    "    rel_errors.append(rel_err)\n",
    "    return (abs_err, rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_to_onnx(i, x, abs_errors,rel_errors, t0_list, t1_list, t2_list, t3_list, s_list):\n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "   \n",
    "    print(\"converting for batch: \", i)\n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_h = val_h.data #tuple([each.data for each in val_h])\n",
    "    #inputs, labels = x.to(device), labels.to(device)\n",
    "\n",
    "    output, val_h = model(x, val_h)\n",
    "    #h0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    #c0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    weight = next(model.parameters()).data\n",
    "    hidden = weight.new(no_layers,batch_size,hidden_dim).zero_().to(device)\n",
    "    h0 = torch.randn(no_layers,batch_size,hidden_dim).cuda()\n",
    "        \n",
    "    #hidden = (h0,c0)\n",
    "    \n",
    "\n",
    "    since_1 = time.time()\n",
    "    out, (hn, cn) = model(x, hidden)\n",
    "    t_elapsed_1 = time.time() - since_1\n",
    "    # Export the model\n",
    "    since_1 = time.time()\n",
    "    \n",
    "    print(len(hidden))\n",
    "    input_names = [\"input\", \"h0\", \"c0\"]\n",
    "    output_names = [\"output\", \"hn\", \"cn\"]\n",
    "\n",
    "    torch.onnx.export(model, (x, hidden), \n",
    "                  \"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name),  export_params=True, verbose=False\n",
    "                      #input_names=input_names, \n",
    "                     # output_names=output_names\n",
    "                     )\n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    onnx_model = onnx.load(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    size2 = os.path.getsize(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    s_list.append(size2)\n",
    "    \n",
    "    def to_numpy(tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    ort_session = onnxruntime.InferenceSession(\"./data/ONNX/torch/model_ft-{}.onnx\".format(model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(x), 'hidden':to_numpy(hidden)})\n",
    "    #ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "    #ort_outs = ort_session.run(None, ort_inputs)\n",
    "        \n",
    "    t_elapsed_3 = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"*********\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    t0_list.append(t_elapsed_0)\n",
    "    t1_list.append(t_elapsed_1)\n",
    "    t2_list.append(t_elapsed_2)\n",
    "    t3_list.append(t_elapsed_3)\n",
    "    \n",
    "    abs_err = np.absolute(to_numpy(out)-ort_outs[0])\n",
    "    rel_err = np.absolute(to_numpy(out)-ort_outs[0])/ np.absolute(ort_outs[0])\n",
    "    abs_errors.append(abs_err)\n",
    "    rel_errors.append(rel_err)\n",
    "    return (abs_err, rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lets_convert(data):\n",
    "    since = time.time()\n",
    "    list_converted = []\n",
    "    t0_list = []\n",
    "    t1_list = []\n",
    "    t2_list = []\n",
    "    t3_list = []\n",
    "    s_list = []\n",
    "    abs_errors = []\n",
    "    rel_errors = []\n",
    "    for i, (inputs, labels) in enumerate(data):\n",
    "        torch.cuda.empty_cache()\n",
    "        #images = images.cuda()\n",
    "        inputs = inputs.to(device)\n",
    "        list_converted.append(gru_to_onnx(i, inputs, abs_errors,rel_errors, t0_list, t1_list, t2_list, t3_list, s_list))\n",
    "        if i == 10:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s,  Loading Pytorch: {}, Pytorch time: {:.4f}, conversion time: {:.4f}, onnx runtime: {:.4f}'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60, np.mean(np.array(t0_list)), np.mean(np.array(t1_list)), np.mean(np.array(t2_list)), np.mean(np.array(t3_list))) )\n",
    "    \n",
    "    #return list_converted\n",
    "    return list_converted, abs_errors, rel_errors, 'Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60), np.mean(np.array(t0_list)), np.mean(np.array(t1_list)), np.mean(np.array(t2_list)), np.mean(np.array(t3_list)), np.mean(np.array(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  1\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 2s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0181, conversion time: 0.1051, onnx runtime: 0.0317\n",
      "1 1\n",
      "################ Batch size:  5\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 4s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0228, conversion time: 0.1307, onnx runtime: 0.1353\n",
      "5 5\n",
      "################ Batch size:  10\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 5s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0233, conversion time: 0.1299, onnx runtime: 0.2312\n",
      "10 10\n",
      "################ Batch size:  20\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 7s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0356, conversion time: 0.1417, onnx runtime: 0.4146\n",
      "20 20\n",
      "################ Batch size:  30\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 9s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0358, conversion time: 0.1429, onnx runtime: 0.5627\n",
      "30 30\n",
      "################ Batch size:  40\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 11s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0354, conversion time: 0.1407, onnx runtime: 0.7459\n",
      "40 40\n",
      "################ Batch size:  50\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 12s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0330, conversion time: 0.1261, onnx runtime: 0.9080\n",
      "50 50\n",
      "################ Batch size:  60\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 15s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0324, conversion time: 0.1247, onnx runtime: 1.1172\n",
      "60 60\n",
      "################ Batch size:  70\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 16s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0224, conversion time: 0.1103, onnx runtime: 1.2850\n",
      "70 70\n",
      "################ Batch size:  80\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 18s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0227, conversion time: 0.1109, onnx runtime: 1.4111\n",
      "80 80\n",
      "################ Batch size:  90\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 19s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0228, conversion time: 0.1115, onnx runtime: 1.5240\n",
      "90 90\n",
      "################ Batch size:  100\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 20s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0249, conversion time: 0.1137, onnx runtime: 1.6569\n",
      "100 100\n",
      "################ Batch size:  150\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 27s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0230, conversion time: 0.1105, onnx runtime: 2.3020\n",
      "150 150\n",
      "################ Batch size:  200\n",
      "converting for batch:  0\n",
      "2\n",
      "*********\n",
      "converting for batch:  1\n",
      "2\n",
      "*********\n",
      "converting for batch:  2\n",
      "2\n",
      "*********\n",
      "converting for batch:  3\n",
      "2\n",
      "*********\n",
      "converting for batch:  4\n",
      "2\n",
      "*********\n",
      "converting for batch:  5\n",
      "2\n",
      "*********\n",
      "converting for batch:  6\n",
      "2\n",
      "*********\n",
      "converting for batch:  7\n",
      "2\n",
      "*********\n",
      "converting for batch:  8\n",
      "2\n",
      "*********\n",
      "converting for batch:  9\n",
      "2\n",
      "*********\n",
      "converting for batch:  10\n",
      "2\n",
      "*********\n",
      "Conversion complete in 0m 35s,  Loading Pytorch: 0.0414278507232666, Pytorch time: 0.0340, conversion time: 0.1218, onnx runtime: 2.9324\n",
      "200 200\n"
     ]
    }
   ],
   "source": [
    "#list_converted = _lets_convert(valid_loader)\n",
    "model_name = 'GRU-torch'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pandas as pd \n",
    "for batch_size in [1, 5,10,20,30,40,50,60,70,80,90,100,150,200]:\n",
    "    print(\"################ Batch size: \", batch_size)\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    \n",
    "    # dataloaders\n",
    "    #batch_size = 100\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    # specify the image classes\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    list_converted, abs_errors, rel_errors, total_time, t0, t1, t2, t3, file_size = _lets_convert(valid_loader)\n",
    "    for i in range(len(abs_errors)):\n",
    "        if i == 0:\n",
    "            abs_array = abs_errors[i]\n",
    "            rel_array = rel_errors[i]\n",
    "        else:\n",
    "            np.append(abs_array, abs_errors[i])\n",
    "            np.append(rel_array, rel_errors[i])\n",
    "            \n",
    "    abs_list = []\n",
    "    rel_list = []\n",
    "    model_list = []\n",
    "    batch_list = []\n",
    "    summary_list = ['Modelsize:{}, Conversion: {}, Loading: {}, t1: {}, conversion time: {}, onnx runtime: {}, onnx filesize: {}'.format(size0, total_time, t0, t1, t2, t3, file_size)]\n",
    "    for i in range(len(abs_array)):\n",
    "        abs_list.append(abs_array[i])\n",
    "        rel = rel_array[i]\n",
    "        if rel == np.inf or rel == -np.inf:\n",
    "            rel = 0.0\n",
    "        rel_list.append(rel)\n",
    "        batch_list.append(batch_size)\n",
    "        model_list.append(model_name)\n",
    "        if i >= len(summary_list):\n",
    "            summary_list.append('')\n",
    "    print(len(summary_list), len(rel_list))\n",
    "    data = pd.DataFrame({'model':model_list,'batch_size': batch_list, 'abs_errors':abs_list, 'rel_errors':rel_list, 'summary': summary_list})\n",
    "    data.to_csv('./data/errors/torch/tf_errors_{}_{}.csv'.format(model_name, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'GRU-IMDb'\n",
    "class GRURNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5, batch_size=100):\n",
    "        #super(SentimentRNN,self).__init__()\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        gru_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(gru_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.no_layers,batch_size,self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "        \n",
    "    #def init_hidden(self, batch_size):\n",
    "    #    ''' Initializes hidden state '''\n",
    "    #    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "    #    # initialized to zero, for hidden state and cell state of LSTM\n",
    "    #    h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "    #    c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "    #    hidden = (h0,c0)\n",
    "    #    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRURNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5, batch_size=100)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "model_ft = train_rnn(model, epochs, optimizer,criterion, train_loader, valid_loader, 'GRU', batch_size=100)\n",
    "#torch.save(model, './data/23-08-2021/torch_{}_{}.pth'.format(model_name, date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring back the Pytorch model using code generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from onnx_pytorch import code_gen\n",
    "#code_gen.gen(\"./data/ONNX/torch/model_ft-torch_vgg_cifar10_2021-08-23-17:19:32.onnx\", \"./data/ONNX/torch/restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnx2pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onnx_model = onnx.load(\"./data/ONNX/torch/model_ft-torch_vgg_cifar10_2021-08-23-17:19:32.onnx\")\n",
    "#pytorch_model = ConvertModel(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try to prepare the data the same way for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( './data/IMDb/IMDB Dataset.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('./data/IMDb/IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "28735    [well, i, musician, i, thought, maybe, i, chec...\n",
      "45109    [ni, supporter, hard, objectively, review, mov...\n",
      "38477    [this, probably, worst, movie, i, ever, seen, ...\n",
      "16542    [oh, beautiful, oh, tearful, gut, wrenching, m...\n",
      "28202    [the, problem, movie, funny, scary, dramatic, ...\n",
      "                               ...                        \n",
      "33555    [ever, notice, many, really, bad, films, attra...\n",
      "27115    [production, line, collection, fart, jokes, pr...\n",
      "31289    [i, wonder, i, could, take, sitting, whole, mu...\n",
      "23363    [i, friend, rented, movie, we, found, movie, s...\n",
      "6617     [this, beautiful, story, elder, son, coming, h...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "16477    [at, first, look, plot, tagline, i, figured, c...\n",
      "4044     [this, movie, last, straw, list, films, i, see...\n",
      "33646    [i, loved, movie, much, i, big, fan, amanda, b...\n",
      "34497    [the, mother, extraordinary, piece, film, maki...\n",
      "5332     [like, many, i, great, fan, real, thing, noir,...\n",
      "                               ...                        \n",
      "21988    [i, find, little, thats, good, say, film, i, s...\n",
      "22432    [i, saw, movie, showtime, wee, hours, night, i...\n",
      "23389    [i, understand, people, would, praise, garbage...\n",
      "16978    [the, primary, aspect, film, people, miss, luh...\n",
      "38491    [this, luminously, photographed, unusually, we...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "28735    0\n",
      "45109    0\n",
      "38477    0\n",
      "16542    1\n",
      "28202    0\n",
      "        ..\n",
      "33555    0\n",
      "27115    0\n",
      "31289    1\n",
      "23363    0\n",
      "6617     1\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "16477    0\n",
      "4044     0\n",
      "33646    1\n",
      "34497    1\n",
      "5332     1\n",
      "        ..\n",
      "21988    0\n",
      "22432    1\n",
      "23389    0\n",
      "16978    1\n",
      "38491    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[   16     1  5050 ...     0     0     0]\n",
      " [23771 14995   160 ...     0     0     0]\n",
      " [    9   146   152 ...     0     0     0]\n",
      " ...\n",
      " [    1   496     1 ...  7280    71  2792]\n",
      " [    1   339  1473 ...     0     0     0]\n",
      " [    9   218    13 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  289    23    78 ...  1424   977     8]\n",
      " [    9     3   142 ...     0     0     0]\n",
      " [    1   330     3 ...     0     0     0]\n",
      " ...\n",
      " [    1   288    21 ...   820     1    47]\n",
      " [    2  4183  1166 ...   659    37  1964]\n",
      " [    9 44409  3776 ...   405  3424  6953]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.as_tensor(np.array(x_train).astype('int32')), torch.as_tensor(np.array(y_train).astype('int32')))\n",
    "valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 200\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([200, 130])\n",
      "Sample input: \n",
      " tensor([[    1,   578,   649,  ...,     0,     0,     0],\n",
      "        [  441,   529,   794,  ...,  1765,    50, 14238],\n",
      "        [  479,   228,     3,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  204,     6,    35,  ...,     0,     0,     0],\n",
      "        [  349,     6,   920,  ...,     0,     0,     0],\n",
      "        [   15,   176,  2440,  ...,   420,   299, 16866]], dtype=torch.int32)\n",
      "Sample input: \n",
      " tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        #super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-82964f04c321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#extra 1 for padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "model_name = 'LSTM-IMDb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (encoder): Embedding(92252, 128)\n",
       "  (lstm): LSTM(128, 64, num_layers=2, bidirectional=True)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "input_dim = total_words\n",
    "model = BiLSTM(input_dim, embedding_dim, hidden_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm-IMDb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-45489e0bbc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GRU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-15e4f762f8da>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(model, epochs, optimizer, criterion, train_loader, valid_loader, type_)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# we'd backprop through the entire training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GRU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;31m# GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "model_ft = train_rnn(model, epochs, optimizer,criterion, train_loader, valid_loader, 'GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
