{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (37500,)\n",
      "shape of test data is (12500,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['review'].values,df['sentiment'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mosesopenja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ7klEQVR4nO3df6zddX3H8edr1B8M1BZxNw0lK8ZGw2Qi3kCNZrlAVgougyXESIhUx+wSIdGkySxbNjbRBJOhG8wRu1mBhInMH2uDaNdVbox/gBRFyg9ZK5bQBui0BVY0urr3/jifi2f1tvf23Nt77tc+H8nJ+X4/38/3e17ncujrnu/53ntTVUiSjm2/MewAkqThswwkSZaBJMkykCRhGUiSgAXDDjCok08+uZYuXTrQvi+++CInnHDC7AaaI13N3tXc0N3sXc0NZj+aHnjggR9V1esOHu9sGSxdupStW7cOtO/4+DhjY2OzG2iOdDV7V3NDd7N3NTeY/WhK8uRk454mkiRZBpIky0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSHf4J5JnYtvt53rf2q3P+uDuvf9ecP6YkTYfvDCRJloEkyTKQJGEZSJKwDCRJWAaSJCwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJYhplkOTUJPckeTTJI0k+1MZPSrI5yfZ2v6iNJ8mNSXYkeSjJWX3HWtXmb0+yqm/8bUm2tX1uTJKj8WQlSZObzjuDA8CaqjodWA5cleR0YC2wpaqWAVvaOsCFwLJ2Ww3cDL3yAK4FzgHOBq6dKJA25wN9+62c+VOTJE3XlGVQVU9X1Xfa8n8DjwGnABcDt7ZptwKXtOWLgduq515gYZLFwAXA5qraW1X7gM3Ayrbt1VV1b1UVcFvfsSRJc2DBkUxOshR4K3AfMFJVT7dNzwAjbfkU4Km+3Xa1scON75pkfLLHX03v3QYjIyOMj48fSfyXjBwPa844MNC+MzFo3n779++flePMta7mhu5m72puMPswTLsMkpwIfAn4cFW90H9av6oqSR2FfP9PVa0D1gGMjo7W2NjYQMe56fYN3LDtiHpwVuy8fGzGxxgfH2fQ5z1MXc0N3c3e1dxg9mGY1tVESV5Grwhur6ovt+Fn2yke2v2eNr4bOLVv9yVt7HDjSyYZlyTNkelcTRTgs8BjVfXJvk0bgYkrglYBG/rGr2hXFS0Hnm+nkzYBK5Isah8crwA2tW0vJFneHuuKvmNJkubAdM6VvAN4L7AtyYNt7M+B64E7k1wJPAm8u227G7gI2AH8BHg/QFXtTXIdcH+b99Gq2tuWPwjcAhwPfK3dJElzZMoyqKpvAYe67v/8SeYXcNUhjrUeWD/J+FbgzVNlkSQdHf4EsiTJMpAkWQaSJCwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJWAaSJCwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiSmUQZJ1ifZk+ThvrG/TrI7yYPtdlHftmuS7EjyeJIL+sZXtrEdSdb2jZ+W5L42/oUkL5/NJyhJmtp03hncAqycZPxTVXVmu90NkOR04D3A77R9/jHJcUmOAz4NXAicDlzW5gJ8oh3rDcA+4MqZPCFJ0pGbsgyq6pvA3mke72Lgjqr6WVX9ENgBnN1uO6rqiar6OXAHcHGSAOcBX2z73wpccmRPQZI0UwtmsO/VSa4AtgJrqmofcApwb9+cXW0M4KmDxs8BXgs8V1UHJpn/K5KsBlYDjIyMMD4+PlDwkeNhzRkHpp44ywbN22///v2zcpy51tXc0N3sXc0NZh+GQcvgZuA6oNr9DcAfz1aoQ6mqdcA6gNHR0RobGxvoODfdvoEbts2kBwez8/KxGR9jfHycQZ/3MHU1N3Q3e1dzg9mHYaB/Eavq2YnlJP8E3NVWdwOn9k1d0sY4xPiPgYVJFrR3B/3zJUlzZKBLS5Ms7lv9I2DiSqONwHuSvCLJacAy4NvA/cCyduXQy+l9yLyxqgq4B7i07b8K2DBIJknS4KZ8Z5Dk88AYcHKSXcC1wFiSM+mdJtoJ/ClAVT2S5E7gUeAAcFVV/aId52pgE3AcsL6qHmkP8RHgjiQfA74LfHa2npwkaXqmLIOqumyS4UP+g11VHwc+Psn43cDdk4w/Qe9qI0nSkPgTyJIky0CSZBlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJWAaSJCwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKYRhkkWZ9kT5KH+8ZOSrI5yfZ2v6iNJ8mNSXYkeSjJWX37rGrztydZ1Tf+tiTb2j43JslsP0lJ0uEtmMacW4B/AG7rG1sLbKmq65OsbesfAS4ElrXbOcDNwDlJTgKuBUaBAh5IsrGq9rU5HwDuA+4GVgJfm/lTm3+Wrv3qjI+x5owDvG+A4+y8/l0zfmxJv76mfGdQVd8E9h40fDFwa1u+Fbikb/y26rkXWJhkMXABsLmq9rYC2AysbNteXVX3VlXRK5xLkCTNqUE/Mxipqqfb8jPASFs+BXiqb96uNna48V2TjEuS5tB0ThMdVlVVkpqNMFNJshpYDTAyMsL4+PhAxxk5vne6pYsGzT7o12q27N+/f+gZBtXV7F3NDWYfhkHL4Nkki6vq6XaqZ08b3w2c2jdvSRvbDYwdND7expdMMn9SVbUOWAcwOjpaY2Njh5p6WDfdvoEbts24B4dizRkHBsq+8/Kx2Q9zBMbHxxn0v9ewdTV7V3OD2Ydh0NNEG4GJK4JWARv6xq9oVxUtB55vp5M2ASuSLGpXHq0ANrVtLyRZ3q4iuqLvWJKkOTLlt5hJPk/vu/qTk+yid1XQ9cCdSa4EngTe3abfDVwE7AB+ArwfoKr2JrkOuL/N+2hVTXwo/UF6VywdT+8qol/LK4kkaT6bsgyq6rJDbDp/krkFXHWI46wH1k8yvhV481Q5JElHjz+BLEmyDCRJloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJWAaSJCwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJLEDMsgyc4k25I8mGRrGzspyeYk29v9ojaeJDcm2ZHkoSRn9R1nVZu/PcmqmT0lSdKRmo13BudW1ZlVNdrW1wJbqmoZsKWtA1wILGu31cDN0CsP4FrgHOBs4NqJApEkzY2jcZroYuDWtnwrcEnf+G3Vcy+wMMli4AJgc1Xtrap9wGZg5VHIJUk6hFTV4DsnPwT2AQV8pqrWJXmuqha27QH2VdXCJHcB11fVt9q2LcBHgDHglVX1sTb+l8BPq+pvJ3m81fTeVTAyMvK2O+64Y6Dce/Y+z7M/HWjXoRs5noGyn3HKa2Y/zBHYv38/J5544lAzDKqr2buaG8x+NJ177rkP9J3JecmCGR73nVW1O8lvAZuTfL9/Y1VVksHb5iBVtQ5YBzA6OlpjY2MDHeem2zdww7aZPvXhWHPGgYGy77x8bPbDHIHx8XEG/e81bF3N3tXcYPZhmNFpoqra3e73AF+hd87/2Xb6h3a/p03fDZzat/uSNnaocUnSHBm4DJKckORVE8vACuBhYCMwcUXQKmBDW94IXNGuKloOPF9VTwObgBVJFrUPjle0MUnSHJnJuZIR4Cu9jwVYAPxLVX09yf3AnUmuBJ4E3t3m3w1cBOwAfgK8H6Cq9ia5Dri/zftoVe2dQS5J0hEauAyq6gngLZOM/xg4f5LxAq46xLHWA+sHzSJJmhl/AlmSZBlIkiwDSRKWgSQJy0CShGUgScIykCRhGUiSsAwkSVgGkiQsA0kSloEkCctAkoRlIEli5n/2Uh2xdO1Xh/K4O69/11AeV9KR8Z2BJMkykCRZBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJWAaSJCwDSRKWgSQJy0CShGUgScK/Z6CjbOLvKKw54wDvm+O/qeDfUpCmz3cGkiTLQJJkGUiSsAwkScyjMkiyMsnjSXYkWTvsPJJ0LJkXVxMlOQ74NPD7wC7g/iQbq+rR4SZTly2dpauXjvRKKK9iUhfNl3cGZwM7quqJqvo5cAdw8ZAzSdIxI1U17AwkuRRYWVV/0tbfC5xTVVcfNG81sLqtvhF4fMCHPBn40YD7DltXs3c1N3Q3e1dzg9mPpt+uqtcdPDgvThNNV1WtA9bN9DhJtlbV6CxEmnNdzd7V3NDd7F3NDWYfhvlymmg3cGrf+pI2JkmaA/OlDO4HliU5LcnLgfcAG4ecSZKOGfPiNFFVHUhyNbAJOA5YX1WPHMWHnPGppiHqavau5obuZu9qbjD7nJsXHyBLkoZrvpwmkiQNkWUgSTq2ymC+/8qLJOuT7EnycN/YSUk2J9ne7he18SS5sT2Xh5KcNcTcpya5J8mjSR5J8qEOZX9lkm8n+V7L/jdt/LQk97WMX2gXNpDkFW19R9u+dFjZW57jknw3yV0dy70zybYkDybZ2sbm/eul5VmY5ItJvp/ksSRv70r2wzlmyqDvV15cCJwOXJbk9OGm+hW3ACsPGlsLbKmqZcCWtg6957Gs3VYDN89RxskcANZU1enAcuCq9rXtQvafAedV1VuAM4GVSZYDnwA+VVVvAPYBV7b5VwL72vin2rxh+hDwWN96V3IDnFtVZ/Zdk9+F1wvA3wNfr6o3AW+h9/XvSvZDq6pj4ga8HdjUt34NcM2wc02ScynwcN/648DitrwYeLwtfwa4bLJ5w74BG+j9nqlOZQd+E/gOcA69nyBdcPBrh94Vb29vywvavAwp7xJ6//CcB9wFpAu5W4adwMkHjc371wvwGuCHB3/tupB9qtsx884AOAV4qm99Vxub70aq6um2/Aww0pbn5fNppx/eCtxHR7K3Uy0PAnuAzcAPgOeq6kCb0p/vpext+/PAa+c08C/9HfBnwP+29dfSjdwABfx7kgfar5mBbrxeTgP+C/hcOz33z0lOoBvZD+tYKoPOq963FvP2WuAkJwJfAj5cVS/0b5vP2avqF1V1Jr3vtM8G3jTcRFNL8gfAnqp6YNhZBvTOqjqL3mmUq5L8Xv/Gefx6WQCcBdxcVW8FXuSXp4SAeZ39sI6lMujqr7x4NsligHa/p43Pq+eT5GX0iuD2qvpyG+5E9glV9RxwD73TKwuTTPxQZn++l7K37a8Bfjy3SQF4B/CHSXbS+y2/59E7lz3fcwNQVbvb/R7gK/RKuAuvl13Arqq6r61/kV45dCH7YR1LZdDVX3mxEVjVllfROx8/MX5Fu1phOfB839vUOZUkwGeBx6rqk32bupD9dUkWtuXj6X3W8Ri9Uri0TTs4+8RzuhT4RvtOcE5V1TVVtaSqltJ7LX+jqi5nnucGSHJCkldNLAMrgIfpwOulqp4BnkryxjZ0PvAoHcg+pWF/aDGXN+Ai4D/pnRP+i2HnmSTf54Gngf+h9x3IlfTO624BtgP/AZzU5obe1VE/ALYBo0PM/U56b4sfAh5st4s6kv13ge+27A8Df9XGXw98G9gB/Cvwijb+yra+o21//Tx43YwBd3Uld8v4vXZ7ZOL/xS68XlqeM4Gt7TXzb8CirmQ/3M1fRyFJOqZOE0mSDsEykCRZBpIky0CShGUgScIykCRhGUiSgP8DhScHdIhzg90AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    37500.000000\n",
       "mean        69.042800\n",
       "std         47.860823\n",
       "min          0.000000\n",
       "25%         39.000000\n",
       "50%         54.000000\n",
       "75%         84.000000\n",
       "max        653.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have very less number of reviews with length > 500.\n",
    "#So we will consideronly those below it.\n",
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([128, 500])\n",
      "Sample input: \n",
      " tensor([[  0,   0,   0,  ..., 864, 456,  11],\n",
      "        [  0,   0,   0,  ..., 129, 165,  56],\n",
      "        [  0,   0,   0,  ...,  43,  22,   4],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ...,  41, 532,  57],\n",
      "        [  0,   0,   0,  ..., 175, 155,  67],\n",
      "        [  0,   0,   0,  ..., 419,   1, 253]])\n",
      "Sample input: \n",
      " tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_round = 10\n",
    "training_id = training_round\n",
    "model_short_name = 'lstm'\n",
    "framework = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        #super(SentimentRNN,self).__init__()\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMRNN(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "model_name = 'torch_state_lstm-imdb_2021-11-03_{}'.format(training_id)\n",
    "model.load_state_dict(torch.load(path+model_name+'.pb', map_location=torch.device('cpu')))\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "\n",
    "size0 = os.path.getsize(path+model_name+'.pb')\n",
    "size0\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.5.0 detected. Last version known to be fully compatible is 2.3.1 .\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import coremltools\n",
    "import time\n",
    "\n",
    "from six import string_types as _string_types\n",
    "from coremltools import TensorType\n",
    "#from coremltools.converters.mil.testing_reqs import _converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/onnx/'\n",
    "coreml_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/coremltools/'\n",
    "error_path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/errors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "def trace_model(model, input_data):\n",
    "    model.eval()\n",
    "    if isinstance(input_data, list):\n",
    "        input_data = tuple(input_data)\n",
    "    torch_model = torch.jit.trace(model, input_data)\n",
    "    return torch_model\n",
    "def _flatten(object):\n",
    "    flattened_list = []\n",
    "    for item in object:\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            flattened_list.extend(_flatten(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "def flatten_and_detach_torch_results(torch_results):\n",
    "    if isinstance(torch_results, (list, tuple)):\n",
    "        #return [x.detach().numpy() for x in _flatten(torch_results)]\n",
    "        return [x.cpu().numpy() for x in _flatten(torch_results)]\n",
    "    # Do not need to flatten\n",
    "    return [torch_results.detach().numpy()]\n",
    "def convert_to_coreml_inputs(input_description, inputs):\n",
    "    \"\"\"Convenience function to combine a CoreML model's input description and\n",
    "    set of raw inputs into the format expected by the model's predict function.\n",
    "    \"\"\"\n",
    "    flattened_inputs = _flatten(inputs)\n",
    "    coreml_inputs = {\n",
    "        str(x): to_numpy(inp).astype(np.float64) for x, inp in zip(input_description, flattened_inputs)\n",
    "    }\n",
    "    #str(x): to_numpy(inp).numpy() for x, inp in zip(input_description, flattened_inputs)\n",
    "    return coreml_inputs\n",
    "from coremltools.converters.mil.mil import types\n",
    "def _convert_to_inputtype(inputs): \n",
    "    if isinstance(inputs, list): \n",
    "        return [_convert_to_inputtype(x) for x in inputs]\n",
    "    elif isinstance(inputs, tuple):\n",
    "        return tuple([_convert_to_inputtype(x) for x in inputs])\n",
    "        #return tuple([coremltools.TensorType(shape=x.shape) for x in inputs])\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        return coremltools.TensorType(shape=inputs.shape, dtype=types.int64)\n",
    "    else:\n",
    "        raise ValueError(\"Unable to parse type {} into InputType.\".format(type(inputs)))\n",
    "        \n",
    "def model_scores(y_test, test_predict):\n",
    "    correct_ = np.sum(y_test == test_predict)\n",
    "    accuracy  = correct_*100./np.sum(y_test == y_test)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, y, data_writer_run, batch_size):\n",
    "    \n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "    print(\"converting for batch: \", i)\n",
    "    y = to_numpy(y.squeeze())\n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    ### Original Model\n",
    "    torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "    #inputs, labels = x.to(device), labels.to(device)\n",
    "\n",
    "    output, val_h = model(x, val_h)\n",
    "    h0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    c0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    hidden = (h0,c0)\n",
    "    \n",
    "    input_x = (x, (h0, c0))\n",
    "    traced_model = trace_model(model, input_x)\n",
    "    if isinstance(traced_model, _string_types):\n",
    "        torch_model = torch.jit.load(traced_model)\n",
    "    else:\n",
    "        torch_model = traced_model\n",
    "    if not isinstance(input_x, (list, tuple)):\n",
    "        input_x = [input_x]\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    torch_out, (hn, cn) = model(x, (h0, c0))\n",
    "    inference_time_original = time.time() - since_1\n",
    "    y0 = to_numpy(torch.round(torch_out.squeeze()))\n",
    "    accuracy_original = model_scores(y, y0)\n",
    "    \n",
    "    \n",
    "    # ONNX Model\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    \n",
    "    \n",
    "    input_names = [\"input\", \"h0\", \"c0\"]\n",
    "    output_names = [\"output\", \"hn\", \"cn\"]\n",
    "\n",
    "    torch.onnx.export(model, (x, (h0, c0)), \n",
    "                  onnx_path+framework+\"/{}/{}-v2.onnx\".format(model_short_name, model_name),\n",
    "                      input_names=input_names, output_names=output_names)\n",
    "    t_onnx_conversion = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    since_1 = time.time()\n",
    "    onnx_model = onnx.load(onnx_path+framework+\"/{}/{}-v2.onnx\".format(model_short_name, model_name))\n",
    "    load_time_onnx = time.time() - since_1\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    size_onnx = os.path.getsize(onnx_path+framework+\"/{}/{}-v2.onnx\".format(model_short_name, model_name))\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}-v2.onnx\".format(model_short_name, model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(x), 'h0':to_numpy(h0), 'c0':to_numpy(c0)})\n",
    "    inference_time_onnx = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    \n",
    "    ####### Mis-classification ONNX ######################################\n",
    "    ort_outs_tensor = torch.from_numpy(ort_outs[0]) \n",
    "    y2 = to_numpy(torch.round(ort_outs_tensor.squeeze()))\n",
    "    #y2 = to_categorical(np.argmax(ort_outs[0], 1), num_classes = 10)\n",
    "    #correct_onnx = np.sum(y2 == y)\n",
    "    accuracy_onnx = model_scores(y, y2)\n",
    "    miss_perc_val_original_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(to_numpy(torch_out), ort_outs[0])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error onnx0: ', e)\n",
    "    encoded_miss_perc_val_original_onnx = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                encoded_miss_perc_val_original_onnx = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error onnx1: ', e)\n",
    "    \n",
    "    miss_perc_val_test_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error onnx2: ', e)\n",
    "    ####### End of mis-classification ONNX ###################################### \n",
    "    \n",
    "    \n",
    "    ## CoreML\n",
    "    \n",
    "    ## Converting the coremltool\n",
    "    input_data_coreml = list(_convert_to_inputtype(input_x))\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    # Using image_input in the inputs parameter:\n",
    "    # Convert to Core ML using the Unified Conversion API.\n",
    "    coreml_model = coremltools.convert(torch_model, inputs=input_data_coreml)\n",
    "    t_conversion_time_coreml = time.time() - since_1\n",
    "    \n",
    "    coreml_inputs = convert_to_coreml_inputs(coreml_model.input_description, input_x)\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model.save(coreml_path+framework+'/{}/{}-v2.mlmodel'.format(model_short_name, model_name))\n",
    "    t_saving_time_coreml = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    #print(name_1)\n",
    "\n",
    "    size_coreml = os.path.getsize(coreml_path+framework+'/{}/{}-v2.mlmodel'.format(model_short_name, model_name))\n",
    "    \n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}-v2.mlmodel\".format(model_short_name, model_name))\n",
    "    load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    since_1 = time.time()\n",
    "    output_dict_test = coreml_model.predict(coreml_inputs)\n",
    "    inference_time_coreml = time.time() - since_1\n",
    "    ####### Mis-classification coreML ######################################\n",
    "    coreml_array_output = output_dict_test['var_56'] #output_dict_test[list(output_dict_test.keys())[0]]\n",
    "    output_tensor = torch.from_numpy(coreml_array_output) \n",
    "    y3 = to_numpy(torch.round(output_tensor.squeeze()))\n",
    "    #correct_coreml = np.sum(to_numpy(y3) == to_numpy(y))\n",
    "    accuracy_coreml = model_scores(y3, y)\n",
    "    \n",
    "    #print(correct_original, correct_coreml, correct_onnx, np.sum(y == y))\n",
    "    ## Part 1\n",
    "    \n",
    "    miss_perc_val_original_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(to_numpy(torch_out), coreml_array_output)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error coreml0: ', e)\n",
    "    \n",
    "    ####### Part 2\n",
    "    #print('default-shape: ',k_predict.shape, 'onnx-shape: ',ort_outs[0].shape, 'coreml-shape: ',output_dict_test['Identity'].shape)\n",
    "    miss_perc_val_original_coreml = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_coreml = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error coreml1: ', e)\n",
    "    miss_perc_val_test_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        flag = 0\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                flag += 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            print('Error coreml2: ', e)\n",
    "    ####### End of mis-classification coreML ######################################\n",
    "    data_writer_run.writerow([model_short_name, framework, training_id, model_name, batch_size, i,'onnx',size0, size_onnx,t_elapsed_0, inference_time_original,t_onnx_conversion, '', load_time_onnx, \n",
    "                          inference_time_onnx,  miss_perc_val_original_runtime,'',  encoded_miss_perc_val_original_onnx, miss_perc_val_test_runtime, '', accuracy_original, accuracy_onnx])\n",
    "\n",
    "\n",
    "    data_writer_run.writerow([model_short_name, framework, training_id, model_name, batch_size, i,'coremltools',size0, size_coreml,t_elapsed_0, inference_time_original,t_conversion_time_coreml, t_saving_time_coreml, load_time_coreml, \n",
    "                          inference_time_coreml,  miss_perc_val_original_runtime2,'',  miss_perc_val_original_coreml, miss_perc_val_test_runtime2, '', accuracy_original, accuracy_coreml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lets_convert(data,  data_writer_run, batch_size): # for cifar10 etc\n",
    "    since = time.time()\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        to_onnx(i, images,labels, data_writer_run, batch_size)\n",
    "        if i == 50: \n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  128\n",
      "converting for batch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:2099: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 496.93 ops/s]\n",
      "Running MIL Common passes:   0%|          | 0/33 [00:00<?, ? passes/s]/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '29', of the source model, has been renamed to 'var_29' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '30', of the source model, has been renamed to 'var_30' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '56', of the source model, has been renamed to 'var_56' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 384.50 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 279.98 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 341.97 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 518.80 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 318.53 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 238.22 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 313.44 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 512.77 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 295.88 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 186.36 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 318.50 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 553.89 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 321.44 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 239.19 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 293.96 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 427.61 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 275.63 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 188.80 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 303.52 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 453.07 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 365.26 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 249.51 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 303.52 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 459.88 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 352.70 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 200.49 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 313.36 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 743.76 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 415.59 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 276.13 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 309.43 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 549.78 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 416.16 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 251.79 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 346.62 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  9\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 542.59 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 398.65 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 261.35 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 322.24 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  10\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 493.51 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 440.49 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 256.78 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 319.45 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  11\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 525.33 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 404.06 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 232.76 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 325.70 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  12\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 541.08 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 424.67 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 274.38 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 340.51 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  13\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 567.28 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 436.28 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 255.02 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 323.35 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  14\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 540.40 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 332.75 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 245.75 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 334.03 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  15\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 411.21 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 401.99 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 221.92 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 349.27 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  16\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 521.77 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 405.82 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 262.02 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 349.84 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  17\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 562.90 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 421.01 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 261.72 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 321.25 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  18\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 581.17 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 412.73 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 260.03 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 348.05 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  19\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 414.91 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 408.28 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 236.86 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 346.75 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  20\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 517.45 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 386.01 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 273.81 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 314.40 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  21\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 527.81 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 428.43 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 274.91 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 333.81 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  22\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 564.96 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 421.36 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 265.80 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 331.07 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  23\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 527.49 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 431.31 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 257.11 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 361.19 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  24\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 544.97 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 391.27 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 256.29 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 337.66 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  25\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 450.35 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 402.91 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 268.74 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 334.72 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  26\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 504.12 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 377.00 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 262.67 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 286.17 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  27\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 612.08 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 393.86 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 250.22 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 331.06 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  28\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 692.16 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 433.88 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 263.42 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 296.06 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  29\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 387.81 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 388.68 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 275.44 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 329.26 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  30\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 489.40 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 409.76 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 253.76 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 323.04 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  31\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 658.07 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 419.50 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 257.91 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 338.09 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  32\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 901.17 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 478.50 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 280.19 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 342.58 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  33\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 988.82 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 467.61 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 308.88 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 343.81 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  34\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 983.08 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 487.73 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 284.93 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 322.60 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  35\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 884.90 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 470.40 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 268.69 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 284.08 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  36\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 897.25 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 452.40 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 202.52 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 303.06 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  37\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 864.99 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 475.14 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 320.72 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 326.70 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  38\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 881.57 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 500.71 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 256.94 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 341.61 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  39\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 866.76 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 471.42 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 315.16 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 344.38 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  40\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 824.07 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 488.83 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 330.70 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 349.52 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  41\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 820.79 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 491.83 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 338.31 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 349.39 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  42\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 922.21 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 452.20 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 325.21 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 338.14 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  43\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 884.61 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 468.28 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 280.29 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 333.00 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  44\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 914.06 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 481.27 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 300.79 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 310.54 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  45\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 875.38 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 479.99 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 323.94 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 349.14 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  46\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 911.06 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 495.61 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 250.04 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 331.36 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  47\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 911.03 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 499.82 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 261.82 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 338.58 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  48\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 1029.55 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 468.65 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 307.95 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 345.22 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  49\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 877.47 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 488.80 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 303.27 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 358.04 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting for batch:  50\n",
      "\n",
      "*********\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tuple detected at graph input. This will be flattened in the converted model.\n",
      "WARNING:root:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting Frontend ==> MIL Ops:  93%|█████████▎| 39/42 [00:00<00:00, 853.00 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 466.83 passes/s]\n",
      "Running MIL Clean up passes: 100%|██████████| 8/8 [00:00<00:00, 317.56 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 56/56 [00:00<00:00, 341.76 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete in 24m 57s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "if not os.path.exists(error_path+framework+\"/{}\".format(model_short_name)):\n",
    "        Path(error_path+framework+\"/{}\".format(model_short_name)).mkdir(parents=True, exist_ok=True)\n",
    "data_file_run = open(error_path+framework+\"/{}/runtime_miss-classification_{}.csv\".format(model_short_name,model_name), mode='w', newline='',\n",
    "                                  encoding='utf-8')\n",
    "data_writer_run = csv.writer(data_file_run, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "data_writer_run.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'round','runtime','original_memory_size', 'runtime_memory_size','original_load_time', 'original_infererence_time','runtime_conversion_time', 'runtime_saving_time', 'runtime_load_time', \n",
    "                          'runtime_inference_time',  'miss_classified_original_runtime_percentage','',  'encoded_miss_classified_original_runtime_percentage','encoded_miss_classified_original_test_runtime_percentage', '', 'accuracy_original', 'accuracy_runtime'])\n",
    "\n",
    "for batch_size in [128]:\n",
    "    print(\"################ Batch size: \", batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    _lets_convert(valid_loader, data_writer_run, batch_size)\n",
    "    #data_writer_acc.writerow([model_short_name,framework, training_id, model_name, batch_size, correct_original, correct_onnx, correct_coreml])\n",
    "data_file_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
