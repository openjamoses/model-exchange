{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (37500,)\n",
      "shape of test data is (12500,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['review'].values,df['sentiment'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAATdElEQVR4nO3df5Bd5X3f8fenkiGObYIwG40soUjGIinQWA47mDSxx4lqEEwn4IQSqbGRHcYyY+jUddNUtJ1C7ZAhtV3PMHFwcKxBTDCyDKGojBwsq8FuPFXQylb1A5BZBBRpZKSAbeLaJcH59o/7bH0sdqXV3tWuhN6vmTP7nO95zjnPZa72s+c8515SVUiSTm7/YLoHIEmafoaBJMkwkCQZBpIkDANJEjBzugcwUWeeeWYtWLBguochSSeUrVu3/nVVDRxaP2HDYMGCBQwNDU33MCTphJLk6dHq3iaSJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRIn8CeQJ8MF/+bO6R6CjjNbP3b1dA8BgP/9kX803UPQcWj+f9xxzI7tlYEkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkhhHGCRZneRAkp2d2ueTbGvLU0m2tfqCJD/obPt0Z58LkuxIMpzk1iRp9TOSbEzyePs56xi8TknSYYznyuAOYGm3UFW/WVWLq2oxcC/wZ53NT4xsq6prO/XbgPcDi9oycsxVwKaqWgRsauuSpCl0xDCoqq8Cz4+2rf11fxVw9+GOkWQOcFpVba6qAu4ErmibLwfWtPaaTl2SNEX6nTN4G/BsVT3eqS1M8o0kX0nytlabC+zt9NnbagCzq2p/a38LmN3nmCRJR6nfby1dzo9fFewH5lfVc0kuAP5rkvPGe7CqqiQ11vYkK4GVAPPnz5/gkCVJh5rwlUGSmcCvA58fqVXVi1X1XGtvBZ4AzgH2AfM6u89rNYBn222kkdtJB8Y6Z1XdXlWDVTU4MDAw0aFLkg7Rz22ifwI8VlX///ZPkoEkM1r7jfQmive020AvJLmozTNcDdzfdlsPrGjtFZ26JGmKjOfR0ruB/wn8bJK9Sa5pm5bx8onjtwPb26Om9wDXVtXI5PMHgT8BhuldMXyx1W8B3pnkcXoBc8vEX44kaSKOOGdQVcvHqL93lNq99B41Ha3/EHD+KPXngCVHGock6djxE8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkhhHGCRZneRAkp2d2k1J9iXZ1pbLOttuSDKcZHeSSzr1pa02nGRVp74wyV+1+ueTnDKZL1CSdGTjuTK4A1g6Sv2TVbW4LRsAkpwLLAPOa/v8UZIZSWYAnwIuBc4Flre+AH/QjvUm4NvANf28IEnS0TtiGFTVV4Hnx3m8y4G1VfViVT0JDAMXtmW4qvZU1d8Ca4HLkwT4VeCetv8a4IqjewmSpH71M2dwfZLt7TbSrFabCzzT6bO31caqvx74TlW9dEh9VElWJhlKMnTw4ME+hi5J6ppoGNwGnA0sBvYDn5isAR1OVd1eVYNVNTgwMDAVp5Skk8LMiexUVc+OtJN8Bnigre4Dzup0nddqjFF/Djg9ycx2ddDtL0maIhO6Mkgyp7P6LmDkSaP1wLIkpyZZCCwCHga2AIvak0On0JtkXl9VBfwFcGXbfwVw/0TGJEmauCNeGSS5G3gHcGaSvcCNwDuSLAYKeAr4AEBV7UqyDngEeAm4rqp+2I5zPfAgMANYXVW72in+LbA2ye8B3wA+O1kvTpI0PkcMg6paPkp5zF/YVXUzcPMo9Q3AhlHqe+g9bSRJmiZ+AlmSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEuMIgySrkxxIsrNT+1iSx5JsT3JfktNbfUGSHyTZ1pZPd/a5IMmOJMNJbk2SVj8jycYkj7efs47B65QkHcZ4rgzuAJYeUtsInF9VPw98E7ihs+2Jqlrclms79duA9wOL2jJyzFXApqpaBGxq65KkKXTEMKiqrwLPH1L7UlW91FY3A/MOd4wkc4DTqmpzVRVwJ3BF23w5sKa113TqkqQpMhlzBr8NfLGzvjDJN5J8JcnbWm0usLfTZ2+rAcyuqv2t/S1g9lgnSrIyyVCSoYMHD07C0CVJ0GcYJPn3wEvAXa20H5hfVW8BPgx8Lslp4z1eu2qow2y/vaoGq2pwYGCgj5FLkrpmTnTHJO8F/imwpP0Sp6peBF5s7a1JngDOAfbx47eS5rUawLNJ5lTV/nY76cBExyRJmpgJXRkkWQr8LvBrVfX9Tn0gyYzWfiO9ieI97TbQC0kuak8RXQ3c33ZbD6xo7RWduiRpihzxyiDJ3cA7gDOT7AVupPf00KnAxvaE6Ob25NDbgY8k+Tvg74Frq2pk8vmD9J5MejW9OYaReYZbgHVJrgGeBq6alFcmSRq3I4ZBVS0fpfzZMfreC9w7xrYh4PxR6s8BS440DknSseMnkCVJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIElinGGQZHWSA0l2dmpnJNmY5PH2c1arJ8mtSYaTbE/yC519VrT+jydZ0alfkGRH2+fWtP+xsiRpaoz3yuAOYOkhtVXApqpaBGxq6wCXAovashK4DXrhAdwIvBW4ELhxJEBan/d39jv0XJKkY2hcYVBVXwWeP6R8ObCmtdcAV3Tqd1bPZuD0JHOAS4CNVfV8VX0b2AgsbdtOq6rNVVXAnZ1jSZKmQD9zBrOran9rfwuY3dpzgWc6/fa22uHqe0epv0ySlUmGkgwdPHiwj6FLkromZQK5/UVfk3GsI5zn9qoarKrBgYGBY306STpp9BMGz7ZbPLSfB1p9H3BWp9+8Vjtcfd4odUnSFOknDNYDI08ErQDu79Svbk8VXQR8t91OehC4OMmsNnF8MfBg2/ZCkovaU0RXd44lSZoCM8fTKcndwDuAM5PspfdU0C3AuiTXAE8DV7XuG4DLgGHg+8D7AKrq+SQfBba0fh+pqpFJ6Q/Se2Lp1cAX2yJJmiLjCoOqWj7GpiWj9C3gujGOsxpYPUp9CDh/PGORJE0+P4EsSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIk+wiDJzybZ1lleSPKhJDcl2depX9bZ54Ykw0l2J7mkU1/aasNJVvX7oiRJR2fmRHesqt3AYoAkM4B9wH3A+4BPVtXHu/2TnAssA84D3gB8Ock5bfOngHcCe4EtSdZX1SMTHZsk6ehMOAwOsQR4oqqeTjJWn8uBtVX1IvBkkmHgwrZtuKr2ACRZ2/oaBpI0RSZrzmAZcHdn/fok25OsTjKr1eYCz3T67G21seovk2RlkqEkQwcPHpykoUuS+g6DJKcAvwZ8oZVuA86mdwtpP/CJfs8xoqpur6rBqhocGBiYrMNK0klvMm4TXQp8vaqeBRj5CZDkM8ADbXUfcFZnv3mtxmHqkqQpMBm3iZbTuUWUZE5n27uAna29HliW5NQkC4FFwMPAFmBRkoXtKmNZ6ytJmiJ9XRkkeQ29p4A+0Cn/5ySLgQKeGtlWVbuSrKM3MfwScF1V/bAd53rgQWAGsLqqdvUzLknS0ekrDKrq/wCvP6T2nsP0vxm4eZT6BmBDP2ORJE2cn0CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiUkIgyRPJdmRZFuSoVY7I8nGJI+3n7NaPUluTTKcZHuSX+gcZ0Xr/3iSFf2OS5I0fpN1ZfArVbW4qgbb+ipgU1UtAja1dYBLgUVtWQncBr3wAG4E3gpcCNw4EiCSpGPvWN0muhxY09prgCs69TurZzNwepI5wCXAxqp6vqq+DWwElh6jsUmSDjEZYVDAl5JsTbKy1WZX1f7W/hYwu7XnAs909t3bamPVJUlTYOYkHOOXq2pfkp8GNiZ5rLuxqipJTcJ5aGGzEmD+/PmTcUhJEpNwZVBV+9rPA8B99O75P9tu/9B+Hmjd9wFndXaf12pj1Q891+1VNVhVgwMDA/0OXZLU9BUGSV6T5HUjbeBiYCewHhh5ImgFcH9rrweubk8VXQR8t91OehC4OMmsNnF8catJkqZAv7eJZgP3JRk51ueq6s+TbAHWJbkGeBq4qvXfAFwGDAPfB94HUFXPJ/kosKX1+0hVPd/n2CRJ49RXGFTVHuDNo9SfA5aMUi/gujGOtRpY3c94JEkT4yeQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCTRRxgkOSvJXyR5JMmuJP+y1W9Ksi/JtrZc1tnnhiTDSXYnuaRTX9pqw0lW9feSJElHa2Yf+74E/Ouq+nqS1wFbk2xs2z5ZVR/vdk5yLrAMOA94A/DlJOe0zZ8C3gnsBbYkWV9Vj/QxNknSUZhwGFTVfmB/a/9NkkeBuYfZ5XJgbVW9CDyZZBi4sG0brqo9AEnWtr6GgSRNkUmZM0iyAHgL8FetdH2S7UlWJ5nVanOBZzq77W21seqjnWdlkqEkQwcPHpyMoUuSmIQwSPJa4F7gQ1X1AnAbcDawmN6Vwyf6PceIqrq9qgaranBgYGCyDitJJ71+5gxI8ip6QXBXVf0ZQFU929n+GeCBtroPOKuz+7xW4zB1SdIU6OdpogCfBR6tqv/Sqc/pdHsXsLO11wPLkpyaZCGwCHgY2AIsSrIwySn0JpnXT3RckqSj18+VwS8B7wF2JNnWav8OWJ5kMVDAU8AHAKpqV5J19CaGXwKuq6ofAiS5HngQmAGsrqpdfYxLknSU+nma6C+BjLJpw2H2uRm4eZT6hsPtJ0k6tvwEsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkjqMwSLI0ye4kw0lWTfd4JOlkclyEQZIZwKeAS4FzgeVJzp3eUUnSyeO4CAPgQmC4qvZU1d8Ca4HLp3lMknTSmDndA2jmAs901vcCbz20U5KVwMq2+r0ku6dgbCeLM4G/nu5BTLd8fMV0D0Ev53tzxI2ZjKP8zGjF4yUMxqWqbgdun+5xvBIlGaqqwekeh3Qo35tT43i5TbQPOKuzPq/VJElT4HgJgy3AoiQLk5wCLAPWT/OYJOmkcVzcJqqql5JcDzwIzABWV9WuaR7Wycbbbzpe+d6cAqmq6R6DJGmaHS+3iSRJ08gwkCQZBnq5JKcn+WBn/Q1J7pnOMenkk+TaJFe39nuTvKGz7U/8loLJ5ZyBXibJAuCBqjp/usciASR5CPidqhqa7rG8UnllcAJKsiDJo0k+k2RXki8leXWSs5P8eZKtSf5Hkp9r/c9OsjnJjiS/l+R7rf7aJJuSfL1tG/kKkFuAs5NsS/Kxdr6dbZ/NSc7rjOWhJINJXpNkdZKHk3yjcyydhNp75rEkd7X36j1JfjLJkvb+2NHeL6e2/rckeSTJ9iQfb7WbkvxOkiuBQeCu9p58ded9d22Sj3XO+94kf9ja727vx21J/rh9B5rGUlUuJ9gCLABeAha39XXAu4FNwKJWeyvw31v7AWB5a18LfK+1ZwKntfaZwDCQdvydh5xvZ2v/K+A/tfYcYHdr/z7w7tY+Hfgm8Jrp/m/lMq3v0QJ+qa2vBv4Dva+dOafV7gQ+BLwe2M2P7lSc3n7eRO9qAOAhYLBz/IfoBcQAve81G6l/Efhl4B8C/w14Vav/EXD1dP93OZ4XrwxOXE9W1bbW3krvH98/Br6QZBvwx/R+WQP8IvCF1v5c5xgBfj/JduDL9L4javYRzrsOuLK1rwJG5hIuBla1cz8E/AQw/+hekl5hnqmqr7X2nwJL6L1vv9lqa4C3A98F/i/w2SS/Dnx/vCeoqoPAniQXJXk98HPA19q5LgC2tPfkEuCN/b+kV67j4kNnmpAXO+0f0vsl/p2qWnwUx/gten9ZXVBVf5fkKXq/xMdUVfuSPJfk54HfpHelAb1g+Y2q8ssDNeLQCcnv0LsK+PFOvQ+dXkjvF/aVwPXArx7FedbS+8PkMeC+qqokAdZU1Q0TGfjJyCuDV44XgCeT/DOA9Ly5bdsM/EZrL+vs81PAgRYEv8KPvs3wb4DXHeZcnwd+F/ipqtreag8C/6L9IyTJW/p9QTrhzU/yi639z4EhYEGSN7Xae4CvJHktvffSBnq3Id/88kMd9j15H72vvF9OLxigd8v0yiQ/DZDkjCSjflunegyDV5bfAq5J8r+AXfzo/wnxIeDD7XbQm+hdlgPcBQwm2QFcTe8vK6rqOeBrSXZ2J+c67qEXKus6tY8CrwK2J9nV1nVy2w1cl+RRYBbwSeB99G5l7gD+Hvg0vV/yD7T3518CHx7lWHcAnx6ZQO5uqKpvA48CP1NVD7faI/TmKL7UjruRH9021Sh8tPQkkOQngR+0y+dl9CaTfdpHx4yPJ594nDM4OVwA/GG7hfMd4LendziSjjdeGUiSnDOQJBkGkiQMA0kShoEkCcNAkgT8PzQVQyNRpWWnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mosesopenja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    37500.000000\n",
       "mean        69.189867\n",
       "std         48.031120\n",
       "min          0.000000\n",
       "25%         39.000000\n",
       "50%         54.000000\n",
       "75%         84.000000\n",
       "max        653.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')\n",
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have very less number of reviews with length > 500.\n",
    "#So we will consideronly those below it.\n",
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 84\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        #super(SentimentRNN,self).__init__()\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_round = 10\n",
    "training_id = training_round\n",
    "model_short_name = 'lstm'\n",
    "framework = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRNN(\n",
       "  (embedding): Embedding(1001, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "model = LSTMRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "model_name = 'torch_state_lstm-imdb_2021-11-03_{}'.format(training_id)\n",
    "model.load_state_dict(torch.load(path+model_name+'.pb', map_location=torch.device('cpu')))\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "\n",
    "size0 = os.path.getsize(path+model_name+'.pb')\n",
    "size0\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "    #inputs, labels = x.to(device), labels.to(device)\n",
    "\n",
    "    #output, val_h = model(x, val_h)\n",
    "    h0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    c0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    hidden = (h0,c0)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    val_acc = 0.0\n",
    "    since = time.time()\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        #images = images.cuda()\n",
    "        print(i)\n",
    "        \n",
    "    \n",
    "        x, (hn, cn) = model(images, (h0, c0))\n",
    "        #x = model(images)\n",
    "        accuracy = acc(x,labels)\n",
    "        val_acc += accuracy\n",
    "        \n",
    "        #value, pred = torch.max(x,0)\n",
    "        #pred = pred.data.cpu()\n",
    "        total += x.size(0)\n",
    "        #correct += torch.sum(pred == labels)\n",
    "        if i == 147:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('accuracy: {}%'.format(val_acc*100./total),'time {:.0f}m {:.0f}s, {}'.format(time_elapsed // 60, time_elapsed % 60, time_elapsed) )\n",
    "    #return correct*100./total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "accuracy: 56.37065637065637% time 8m 38s, 518.4412131309509\n"
     ]
    }
   ],
   "source": [
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "validate(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRURNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5, batch_size=100):\n",
    "        #super(SentimentRNN,self).__init__()\n",
    "        super().__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        gru_out, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(gru_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.no_layers,batch_size,self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "        \n",
    "    #def init_hidden(self, batch_size):\n",
    "    #    ''' Initializes hidden state '''\n",
    "    #    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "    #    # initialized to zero, for hidden state and cell state of LSTM\n",
    "    #    h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "    #    c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "    #    hidden = (h0,c0)\n",
    "    #    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRURNN(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (gru): GRU(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_short_name='gru'\n",
    "model = GRURNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5, batch_size=batch_size)\n",
    "path = '/Volumes/Cisco/Summer2022/onnx-exchange/Train2/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "model_name = 'torch_state_GRU-IMDb_2022-03-11_{}'.format(training_id)\n",
    "model.load_state_dict(torch.load(path+model_name+'.pb', map_location=torch.device('cpu')))\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "\n",
    "size0 = os.path.getsize(path+model_name+'.pb')\n",
    "size0\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, data):\n",
    "    \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_h = val_h.data #tuple([each.data for each in val_h])\n",
    "    #inputs, labels = x.to(device), labels.to(device)\n",
    "\n",
    "    #h0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    #c0 = torch.zeros((no_layers,batch_size,hidden_dim)).to(device)\n",
    "    weight = next(model.parameters()).data\n",
    "    hidden = weight.new(no_layers,batch_size,hidden_dim).zero_().to(device)\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    val_acc = 0.0\n",
    "    list_time = []\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        since = time.time()\n",
    "        #images = images.cuda()\n",
    "        print(i)\n",
    "        \n",
    "    \n",
    "        output, (hn, cn) = model(images, hidden)\n",
    "        #x = model(images)\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        val_acc += accuracy\n",
    "        \n",
    "        #value, pred = torch.max(x,1)\n",
    "        #pred = pred.data.cpu()\n",
    "        total += output.size(0)\n",
    "        #correct += torch.sum(pred == labels)\n",
    "        time_elapsed = time.time() - since\n",
    "        list_time.append(time_elapsed)\n",
    "        \n",
    "        if i == 147:\n",
    "            break\n",
    "    print(np.mean(time_elapsed))\n",
    "    epoch_val_acc = val_acc/len(data.dataset)\n",
    "    \n",
    "    epoch_val_acc2 = val_acc/total\n",
    "    print('accuracy: {}%'.format(epoch_val_acc2*100),'time {:.0f}m {:.0f}s, {}'.format(time_elapsed // 60, time_elapsed % 60, time_elapsed) )\n",
    "    #return correct*100./total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "2.5588490962982178\n",
      "accuracy: 49.670205920205916% time 0m 3s, 2.5588490962982178\n"
     ]
    }
   ],
   "source": [
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "validate(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
