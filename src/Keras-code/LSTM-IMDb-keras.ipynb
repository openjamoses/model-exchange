{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Reviews using LSTM and GRU in Keras\n",
    "\n",
    "### Steps\n",
    "- Load the dataset (50K IMDB Movie Review)\n",
    "- Clean Dataset\n",
    "- Encode Sentiments\n",
    "- Split Dataset\n",
    "- Tokenize and Pad/Truncate Reviews\n",
    "- Build Architecture/Model\n",
    "- Train and Test\n",
    "- Import all the libraries needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( './data/IMDb/IMDB Dataset.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "Declaring the english stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load and Clean Dataset\n",
    "In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.\n",
    "\n",
    "## Encode Sentiments\n",
    "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('./data/IMDb/IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "48330    [haven, played, game, don, bother, this, final...\n",
      "33365    [to, day, i, never, seen, elizabeth, shue, any...\n",
      "4920     [leos, carax, brilliant, one, best, film, came...\n",
      "6206     [one, favorite, movies, ever, along, casablanc...\n",
      "27202    [good, attempt, tackling, unconventional, topi...\n",
      "                               ...                        \n",
      "13542    [s, s, van, dine, must, shrewd, businessman, d...\n",
      "16524    [what, freaking, problem, do, nothing, better,...\n",
      "41064    [forget, jaded, comments, come, this, action, ...\n",
      "2372     [this, movie, definitely, one, finest, kind, a...\n",
      "18284    [being, fan, silent, films, i, looked, forward...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "41302    [if, want, learn, something, spanish, civil, w...\n",
      "20106    [forget, every, spy, movie, ever, seen, life, ...\n",
      "22989    [it, known, whether, marilyn, monroe, ever, me...\n",
      "13332    [can, scarcely, imagine, better, movie, than, ...\n",
      "12266    [la, petit, tourette, pretty, funny, south, pa...\n",
      "                               ...                        \n",
      "20389    [without, question, film, powerful, medium, ev...\n",
      "47710    [this, easily, disappointing, least, gratifyin...\n",
      "25014    [man, year, tells, story, tom, dobbs, robin, w...\n",
      "42276    [a, drama, core, anna, displays, genuine, trut...\n",
      "42981    [this, mad, scientist, creates, half, shark, h...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "48330    1\n",
      "33365    0\n",
      "4920     1\n",
      "6206     1\n",
      "27202    0\n",
      "        ..\n",
      "13542    1\n",
      "16524    1\n",
      "41064    1\n",
      "2372     1\n",
      "18284    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "41302    1\n",
      "20106    1\n",
      "22989    1\n",
      "13332    1\n",
      "12266    1\n",
      "        ..\n",
      "20389    1\n",
      "47710    0\n",
      "25014    1\n",
      "42276    0\n",
      "42981    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the maximum review length, by calculating the mean of all the reviews length (using numpy.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad/Truncate ReviewsÂ¶\n",
    "A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.\n",
    "x_train and x_test is converted into integers using texts_to_sequences method.\n",
    "\n",
    "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.\n",
    "\n",
    "post, pad or truncate the words in the back of a sentence\n",
    "pre, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[10974   162   352 ...  6144 13746  6748]\n",
      " [  282   156     1 ...     0     0     0]\n",
      " [23836 10480   422 ...     0     0     0]\n",
      " ...\n",
      " [  724  5826   709 ...     0     0     0]\n",
      " [    8     3   310 ...     0     0     0]\n",
      " [ 2093   238  1205 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[   55    88   744 ...   156    11  7637]\n",
      " [  724    83  2513 ...     0     0     0]\n",
      " [    7   460   606 ...  5760   110 11843]\n",
      " ...\n",
      " [   52   192   629 ...   136     6   114]\n",
      " [   39   356  1819 ...    14  4738    10]\n",
      " [    8  1032  1502 ...    88   265   152]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build Architecture/Model\n",
    "Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "LSTM Layer: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "\n",
    "Forget Gate, decides information is to be kept or thrown away\n",
    "Input Gate, updates cell state by passing previous output and current input into sigmoid activation function\n",
    "Cell State, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.\n",
    "Ouput Gate, decides the next hidden state and used for predictions\n",
    "Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         11838976  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 12,036,737\n",
      "Trainable params: 12,036,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype='int32')\n",
    "x = layers.Embedding(total_words, 128)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" EMBED_DIM = 32\\nLSTM_OUT = 64\\n\\nmodel = Sequential()\\nmodel.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\\nmodel.add(LSTM(LSTM_OUT))\\nmodel.add(Dense(1, activation='sigmoid'))\\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\\n\\nprint(model.summary())\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "''' EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm-imdb'\n",
    "training_round = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = keras.callbacks.ModelCheckpoint(filepath=\"./data/26-10-2021/Train1/Keras/lstm/tf_{}_{}_{}.h5\".format(model_name,date, training_round),\n",
    "                                              monitor=\"val_accuracy\",\n",
    "                                              mode=\"max\",\n",
    "                                              save_best_only=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 64s 171ms/step - loss: 0.4947 - accuracy: 0.7341 - val_loss: 0.2960 - val_accuracy: 0.8835\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 52s 165ms/step - loss: 0.1589 - accuracy: 0.9438 - val_loss: 0.3141 - val_accuracy: 0.8827\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 52s 166ms/step - loss: 0.0715 - accuracy: 0.9780 - val_loss: 0.3564 - val_accuracy: 0.8727\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 0.5190 - val_accuracy: 0.8724\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.5495 - val_accuracy: 0.8682\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 0.0205 - accuracy: 0.9938 - val_loss: 0.5212 - val_accuracy: 0.8561\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.6974 - val_accuracy: 0.8546\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 52s 168ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.5738 - val_accuracy: 0.8570\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 52s 166ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.8216 - val_accuracy: 0.8597\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.7924 - val_accuracy: 0.8646\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.7454 - val_accuracy: 0.8568\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 57s 183ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.7140 - val_accuracy: 0.8512\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.6990 - val_accuracy: 0.8506\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.7155 - val_accuracy: 0.8586\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.8974 - val_accuracy: 0.8580\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.8093 - val_accuracy: 0.8601\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.9946 - val_accuracy: 0.8587\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.7754 - val_accuracy: 0.8520\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.7901 - val_accuracy: 0.8595\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.7486 - val_accuracy: 0.8595\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.8328 - val_accuracy: 0.8590\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 1.0386 - val_accuracy: 0.8614\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.9575 - val_accuracy: 0.8581\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.9249 - val_accuracy: 0.8539\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.9305 - val_accuracy: 0.8474\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.8372 - val_accuracy: 0.8473\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.8521 - val_accuracy: 0.8600\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.0270 - val_accuracy: 0.8538\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.3501e-04 - accuracy: 1.0000 - val_loss: 1.1028 - val_accuracy: 0.8580\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 6.7477e-04 - accuracy: 0.9999 - val_loss: 1.1594 - val_accuracy: 0.8600\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 1.4685e-04 - accuracy: 1.0000 - val_loss: 1.1770 - val_accuracy: 0.8598\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 4.0720e-04 - accuracy: 1.0000 - val_loss: 1.2093 - val_accuracy: 0.8595\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 3.0251e-04 - accuracy: 1.0000 - val_loss: 1.2426 - val_accuracy: 0.8583\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 1.4225e-05 - accuracy: 1.0000 - val_loss: 1.2934 - val_accuracy: 0.8589\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 8.0610e-06 - accuracy: 1.0000 - val_loss: 1.3338 - val_accuracy: 0.8592\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 52s 167ms/step - loss: 5.6614e-06 - accuracy: 1.0000 - val_loss: 1.3679 - val_accuracy: 0.8583\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 4.2281e-06 - accuracy: 1.0000 - val_loss: 1.3982 - val_accuracy: 0.8580\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 3.4070e-06 - accuracy: 1.0000 - val_loss: 1.4258 - val_accuracy: 0.8579\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 2.5621e-06 - accuracy: 1.0000 - val_loss: 1.4515 - val_accuracy: 0.8579\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 52s 165ms/step - loss: 2.1056e-06 - accuracy: 1.0000 - val_loss: 1.4757 - val_accuracy: 0.8577\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 52s 167ms/step - loss: 1.6625e-06 - accuracy: 1.0000 - val_loss: 1.4989 - val_accuracy: 0.8574\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 1.3451e-06 - accuracy: 1.0000 - val_loss: 1.5214 - val_accuracy: 0.8577\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 1.1558e-06 - accuracy: 1.0000 - val_loss: 1.5432 - val_accuracy: 0.8571\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 9.8119e-07 - accuracy: 1.0000 - val_loss: 1.5643 - val_accuracy: 0.8574\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 55s 174ms/step - loss: 7.7315e-07 - accuracy: 1.0000 - val_loss: 1.5854 - val_accuracy: 0.8569\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 6.7652e-07 - accuracy: 1.0000 - val_loss: 1.6056 - val_accuracy: 0.8570\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 5.6009e-07 - accuracy: 1.0000 - val_loss: 1.6257 - val_accuracy: 0.8572\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 4.5237e-07 - accuracy: 1.0000 - val_loss: 1.6456 - val_accuracy: 0.8576\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 52s 165ms/step - loss: 4.2057e-07 - accuracy: 1.0000 - val_loss: 1.6654 - val_accuracy: 0.8576\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 3.3893e-07 - accuracy: 1.0000 - val_loss: 1.6851 - val_accuracy: 0.8579\n",
      "CPU times: user 1h 56min 32s, sys: 37min 6s, total: 2h 33min 38s\n",
      "Wall time: 44min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history_csv(history_, model_name):\n",
    "    since = time.time()\n",
    "   \n",
    "    data_file = open('./data/26-10-2021/Train1/Keras/lstm/tf_{}_{}_{}.csv'.format(model_name, date, training_round), mode='w+', newline='', encoding='utf-8')\n",
    "    data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['Model','type', 'Dataset', 'Epoch', 'criterion', 'optimizer', 'scheduler','Train_loss', 'Train_acc', \"val_loss\", \"Val_acc\", 'time','Elapse_time','date'])\n",
    "    for epoch_ in history_.epoch:\n",
    "        data_writer.writerow([history_.model,'tensorflow', 'hymenoptera', epoch_, '', \n",
    "                          history_.model.optimizer, '',history_.history['loss'][epoch_], history_.history['accuracy'][epoch_], \n",
    "                          history_.history['val_loss'][epoch_], history_.history['val_accuracy'][epoch_], '','',date])\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_history_csv(history, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./data/26-10-2021/Train1/Keras/lstm/tf_{}_{}_{}.h5\".format(model_name, date, training_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144762352 3685471\n"
     ]
    }
   ],
   "source": [
    "size1 = os.path.getsize(\"./data/23-08-2021/tf_{}.h5\".format(model_name))\n",
    "size1\n",
    "\n",
    "size2 = os.path.getsize(\"./data/23-08-2021/torch_LSTM-IMDb.pth\")\n",
    "print(size1, size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144762352"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/23-08-2021/'\n",
    "since_0 = time.time()\n",
    "#model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "model_name = 'tf_lstm-imdb_2021-09-11-16:49:06'\n",
    "model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "size0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.as_tensor(np.array(x_train).astype('int32')), torch.as_tensor(np.array(y_train).astype('int32')))\n",
    "valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 200\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import time\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, abs_errors,rel_errors, t0_list, t1_list, t2_list, t3_list, s_list):\n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "   \n",
    "    print(\"converting for batch: \", i)\n",
    "    \n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    since_1 = time.time()\n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    with tf.device('/cpu:0'):  \n",
    "        k_predict = model.predict(x)\n",
    "    t_elapsed_1 = time.time() - since_1\n",
    "    # Export the model\n",
    "    since_1 = time.time()\n",
    "    model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model,\n",
    "                input_signature=None, opset=11, custom_ops=None,\n",
    "                custom_op_handlers=None, custom_rewriter=None,\n",
    "                inputs_as_nchw=None, extra_opset=None, shape_override=None,\n",
    "                 target=None, large_model=False, output_path='./data/ONNX/keras/keras-{}.onnx'.format(model_name))\n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    \n",
    "    \n",
    "    onnx_model = onnx.load(\"./data/ONNX/keras/keras-{}.onnx\".format(model_name))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    size2 = os.path.getsize(\"./data/ONNX/keras/keras-{}.onnx\".format(model_name))\n",
    "    s_list.append(size2)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    ort_session = onnxruntime.InferenceSession(\"./data/ONNX/keras/keras-{}.onnx\".format(model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    t_elapsed_3 = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    t0_list.append(t_elapsed_0)\n",
    "    t1_list.append(t_elapsed_1)\n",
    "    t2_list.append(t_elapsed_2)\n",
    "    t3_list.append(t_elapsed_3)\n",
    "    abs_err = np.absolute(k_predict-ort_outs[0])\n",
    "    rel_err = np.absolute(k_predict-ort_outs[0])/ np.absolute(ort_outs[0])\n",
    "    abs_errors.append(abs_err)\n",
    "    rel_errors.append(rel_err)\n",
    "    \n",
    "    return (abs_err, rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "def _lets_convert(data):\n",
    "    since = time.time()\n",
    "    list_converted = []\n",
    "    t0_list = []\n",
    "    t1_list = []\n",
    "    t2_list = []\n",
    "    t3_list = []\n",
    "    s_list = []\n",
    "    abs_errors = []\n",
    "    rel_errors = []\n",
    "    for i, (inputs, labels) in enumerate(data):\n",
    "        #torch.cuda.empty_cache()\n",
    "        #images = images.cuda()\n",
    "        inputs = to_numpy(inputs)\n",
    "        labels = to_numpy(labels)\n",
    "        list_converted.append(to_onnx(i, inputs, abs_errors,rel_errors, t0_list, t1_list, t2_list, t3_list, s_list))\n",
    "        if i == 8:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s,  Loading Pytorch: {}, Pytorch time: {:.4f}, conversion time: {:.4f}, onnx runtime: {:.4f}, onnx filesize: {}'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60, np.mean(np.array(t0_list)), np.mean(np.array(t1_list)), np.mean(np.array(t2_list)), np.mean(np.array(t3_list)), np.mean(np.array(s_list))) )\n",
    "    \n",
    "    return list_converted, abs_errors, rel_errors, 'Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60), np.mean(np.array(t0_list)), np.mean(np.array(t1_list)), np.mean(np.array(t2_list)), np.mean(np.array(t3_list)), np.mean(np.array(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  1\n",
      "converting for batch:  0\n",
      "WARNING:tensorflow:From /store/travail/opmos/conda/envs/tf-gpu/lib/python3.9/site-packages/tf2onnx/tf_loader.py:662: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 17s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2772, conversion time: 14.6218, onnx runtime: 0.0295, onnx filesize: 48256769.777777776\n",
      "1 1\n",
      "################ Batch size:  5\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-aebfcd2c1b2d>:45: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  rel_err = np.absolute(k_predict-ort_outs[0])/ np.absolute(ort_outs[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 31s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.1097, conversion time: 16.3419, onnx runtime: 0.0773, onnx filesize: 48256975.44444445\n",
      "5 5\n",
      "################ Batch size:  10\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 53s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.1361, conversion time: 18.6987, onnx runtime: 0.1009, onnx filesize: 48257045.777777776\n",
      "10 10\n",
      "################ Batch size:  20\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 9s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.1554, conversion time: 20.2985, onnx runtime: 0.1441, onnx filesize: 48257069.333333336\n",
      "20 20\n",
      "################ Batch size:  30\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 22s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.1704, conversion time: 21.7060, onnx runtime: 0.1900, onnx filesize: 48257101.333333336\n",
      "30 30\n",
      "################ Batch size:  40\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 37s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2203, conversion time: 23.1985, onnx runtime: 0.2287, onnx filesize: 48257073.88888889\n",
      "40 40\n",
      "################ Batch size:  50\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 27s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2263, conversion time: 22.1619, onnx runtime: 0.2552, onnx filesize: 48257071.777777776\n",
      "50 50\n",
      "################ Batch size:  60\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 4s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2004, conversion time: 13.0041, onnx runtime: 0.2951, onnx filesize: 48257099.11111111\n",
      "60 60\n",
      "################ Batch size:  70\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 3s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2438, conversion time: 12.8528, onnx runtime: 0.3319, onnx filesize: 48257032.333333336\n",
      "70 70\n",
      "################ Batch size:  80\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 16s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2542, conversion time: 14.1962, onnx runtime: 0.3823, onnx filesize: 48256991.222222224\n",
      "80 80\n",
      "################ Batch size:  90\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 6s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.2797, conversion time: 13.0445, onnx runtime: 0.4352, onnx filesize: 48257269.88888889\n",
      "90 90\n",
      "################ Batch size:  100\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 16s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.3183, conversion time: 14.0275, onnx runtime: 0.4868, onnx filesize: 48257217.11111111\n",
      "100 100\n",
      "################ Batch size:  128\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 32s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.5495, conversion time: 15.3944, onnx runtime: 0.6382, onnx filesize: 48257282.0\n",
      "128 128\n",
      "################ Batch size:  150\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 46s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.4340, conversion time: 16.9535, onnx runtime: 0.7121, onnx filesize: 48257297.666666664\n",
      "150 150\n",
      "################ Batch size:  200\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 2m 59s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.5634, conversion time: 17.9121, onnx runtime: 1.0062, onnx filesize: 48257181.222222224\n",
      "200 200\n",
      "################ Batch size:  250\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 20s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.6621, conversion time: 19.9193, onnx runtime: 1.2076, onnx filesize: 48257213.55555555\n",
      "250 250\n",
      "################ Batch size:  300\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 44s,  Loading Pytorch: 1.5919651985168457, Pytorch time: 0.7957, conversion time: 22.1322, onnx runtime: 1.4757, onnx filesize: 48257270.777777776\n",
      "300 300\n"
     ]
    }
   ],
   "source": [
    "model_name = 'LSTM-keras'\n",
    "import pandas as pd \n",
    "for batch_size in [1, 5,10,20,30,40,50,60,70,80,90,100,128, 150,200, 250, 300]:\n",
    "    print(\"################ Batch size: \", batch_size)\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.as_tensor(np.array(x_train).astype('int32')), torch.as_tensor(np.array(y_train).astype('int32')))\n",
    "    valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "\n",
    "    # dataloaders\n",
    "    #batch_size = batch_size_\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    #list_converted = _lets_convert(test_ds)\n",
    "    list_converted, abs_errors, rel_errors, total_time, t0, t1, t2, t3, file_size  = _lets_convert(valid_loader)\n",
    "    \n",
    "    \n",
    "    for i in range(len(abs_errors)):\n",
    "        if i == 0:\n",
    "            abs_array = abs_errors[i]\n",
    "            rel_array = rel_errors[i]\n",
    "        else:\n",
    "            np.append(abs_array, abs_errors[i])\n",
    "            np.append(rel_array, rel_errors[i])\n",
    "\n",
    "    abs_list = []\n",
    "    rel_list = []\n",
    "    model_list = []\n",
    "    batch_list = []\n",
    "    summary_list = ['Modelsize:{}, Conversion: {}, Loading: {}, t1: {}, conversion time: {}, onnx runtime: {}, onnx filesize: {}'.format(size0, total_time, t0, t1, t2, t3, file_size)]\n",
    "    for i in range(len(abs_array)):\n",
    "        abs_list.append(abs_array[i][0])\n",
    "        rel = rel_array[i][0]\n",
    "        if rel == np.inf or rel == -np.inf:\n",
    "            rel = 0.0\n",
    "        rel_list.append(rel)\n",
    "        batch_list.append(batch_size)\n",
    "        model_list.append(model_name)\n",
    "        if i >= len(summary_list):\n",
    "            summary_list.append('')\n",
    "    print(len(summary_list), len(rel_list))\n",
    "    data = pd.DataFrame({'model':model_list,'batch_size': batch_list, 'abs_errors':abs_list, 'rel_errors':rel_list, 'summary': summary_list})\n",
    "    data.to_csv('./data/errors/keras2/tf_errors_{}_{}.csv'.format(model_name, batch_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 128)         11838976  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 128)         74496     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 11,991,089\n",
      "Trainable params: 11,991,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype='int32')\n",
    "x = layers.Embedding(total_words, 128)(inputs)\n",
    "x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.GRU(64))(x)\n",
    "x = layers.Dense(24,activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "gru_model = keras.Model(inputs, outputs)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gru-imdb'\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = keras.callbacks.ModelCheckpoint(filepath=\"./data/26-10-2021/Train1/Keras/gru/tf_{}_{}_{}.h5\".format(model_name, date, training_round),\n",
    "                                              monitor=\"val_accuracy\",\n",
    "                                              mode=\"max\",\n",
    "                                              save_best_only=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 65s 183ms/step - loss: 0.5046 - accuracy: 0.7276 - val_loss: 0.2743 - val_accuracy: 0.8878\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.1574 - accuracy: 0.9435 - val_loss: 0.3070 - val_accuracy: 0.8855\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0586 - accuracy: 0.9821 - val_loss: 0.4277 - val_accuracy: 0.8736\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 0.0207 - accuracy: 0.9951 - val_loss: 0.5345 - val_accuracy: 0.8749\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.5765 - val_accuracy: 0.8717\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.6764 - val_accuracy: 0.8734\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.7207 - val_accuracy: 0.8703\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.7548 - val_accuracy: 0.8664\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.8181 - val_accuracy: 0.8692\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.7909 - val_accuracy: 0.8703\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.8409 - val_accuracy: 0.8704\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.8360 - val_accuracy: 0.8698\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8065 - val_accuracy: 0.8686\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.9677 - val_accuracy: 0.8723\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 53s 168ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.8754 - val_accuracy: 0.8707\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.7762 - val_accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.8799 - val_accuracy: 0.8700\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.9649 - val_accuracy: 0.8656\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 2.1209e-04 - accuracy: 1.0000 - val_loss: 1.0740 - val_accuracy: 0.8697\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 5.3406e-05 - accuracy: 1.0000 - val_loss: 1.0796 - val_accuracy: 0.8687\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 63s 201ms/step - loss: 6.8331e-06 - accuracy: 1.0000 - val_loss: 1.1102 - val_accuracy: 0.8689\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 4.9995e-06 - accuracy: 1.0000 - val_loss: 1.1367 - val_accuracy: 0.8689\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 66s 210ms/step - loss: 3.7188e-06 - accuracy: 1.0000 - val_loss: 1.1610 - val_accuracy: 0.8691\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 62s 199ms/step - loss: 3.1074e-06 - accuracy: 1.0000 - val_loss: 1.1836 - val_accuracy: 0.8689\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 71s 227ms/step - loss: 2.3169e-06 - accuracy: 1.0000 - val_loss: 1.2052 - val_accuracy: 0.8692\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 67s 215ms/step - loss: 1.8930e-06 - accuracy: 1.0000 - val_loss: 1.2259 - val_accuracy: 0.8694\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 66s 211ms/step - loss: 1.5494e-06 - accuracy: 1.0000 - val_loss: 1.2460 - val_accuracy: 0.8695\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 65s 209ms/step - loss: 1.3303e-06 - accuracy: 1.0000 - val_loss: 1.2657 - val_accuracy: 0.8701\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 1.0595e-06 - accuracy: 1.0000 - val_loss: 1.2851 - val_accuracy: 0.8700\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 69s 219ms/step - loss: 8.9643e-07 - accuracy: 1.0000 - val_loss: 1.3042 - val_accuracy: 0.8699\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 69s 220ms/step - loss: 7.5971e-07 - accuracy: 1.0000 - val_loss: 1.3233 - val_accuracy: 0.8698\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 6.6057e-07 - accuracy: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.8700\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 67s 213ms/step - loss: 5.2676e-07 - accuracy: 1.0000 - val_loss: 1.3607 - val_accuracy: 0.8702\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 67s 214ms/step - loss: 4.4404e-07 - accuracy: 1.0000 - val_loss: 1.3794 - val_accuracy: 0.8705\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 3.9435e-07 - accuracy: 1.0000 - val_loss: 1.3982 - val_accuracy: 0.8703\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 3.2195e-07 - accuracy: 1.0000 - val_loss: 1.4180 - val_accuracy: 0.8704\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 2.6548e-07 - accuracy: 1.0000 - val_loss: 1.4448 - val_accuracy: 0.8704\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 65s 207ms/step - loss: 2.0326e-07 - accuracy: 1.0000 - val_loss: 1.4737 - val_accuracy: 0.8700\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 1.4944e-07 - accuracy: 1.0000 - val_loss: 1.5003 - val_accuracy: 0.8702\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 66s 211ms/step - loss: 1.1493e-07 - accuracy: 1.0000 - val_loss: 1.5241 - val_accuracy: 0.8701\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 9.1401e-08 - accuracy: 1.0000 - val_loss: 1.5461 - val_accuracy: 0.8702\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 68s 216ms/step - loss: 7.8432e-08 - accuracy: 1.0000 - val_loss: 1.5663 - val_accuracy: 0.8701\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 67s 214ms/step - loss: 6.4400e-08 - accuracy: 1.0000 - val_loss: 1.5855 - val_accuracy: 0.8702\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 5.4508e-08 - accuracy: 1.0000 - val_loss: 1.6037 - val_accuracy: 0.8699\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 4.4674e-08 - accuracy: 1.0000 - val_loss: 1.6212 - val_accuracy: 0.8699\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 67s 213ms/step - loss: 4.1201e-08 - accuracy: 1.0000 - val_loss: 1.6385 - val_accuracy: 0.8703\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 66s 212ms/step - loss: 3.5042e-08 - accuracy: 1.0000 - val_loss: 1.6551 - val_accuracy: 0.8701\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 66s 210ms/step - loss: 2.8612e-08 - accuracy: 1.0000 - val_loss: 1.6716 - val_accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 66s 211ms/step - loss: 2.4292e-08 - accuracy: 1.0000 - val_loss: 1.6878 - val_accuracy: 0.8702\n",
      "Epoch 50/50\n",
      "279/313 [=========================>....] - ETA: 6s - loss: 2.2369e-08 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history2 = gru_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history_csv(history_, model_name):\n",
    "    since = time.time()\n",
    "   \n",
    "    data_file = open('./data/26-10-2021/Train1/Keras/gru/tf_{}_{}_{}.csv'.format(model_name, date, training_round), mode='w+', newline='', encoding='utf-8')\n",
    "    data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    data_writer.writerow(['Model','type', 'Dataset', 'Epoch', 'criterion', 'optimizer', 'scheduler','Train_loss', 'Train_acc', \"val_loss\", \"Val_acc\", 'time','Elapse_time','date'])\n",
    "    for epoch_ in history_.epoch:\n",
    "        data_writer.writerow([history_.model,'tensorflow', 'hymenoptera', epoch_, '', \n",
    "                          history_.model.optimizer, '',history_.history['loss'][epoch_], history_.history['accuracy'][epoch_], \n",
    "                          history_.history['val_loss'][epoch_], history_.history['val_accuracy'][epoch_], '','',date])\n",
    "    data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_history_csv(history2, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save(\"./data/26-10-2021/Train1/Keras/gru/tf_{}_{}_{}.h5\".format(model_name, date, training_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
