{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import tf2onnx\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "nb_classes = 10\n",
    "input_shape = (28, 28) #Lenet5\n",
    "#batch_size = 500 # Lenet\n",
    "batch_size = 500 # Lenet\n",
    "input_shape = (28, 28) #Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((50000, 32, 32, 3), (50000, 10))\n",
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 64, 64\n",
    "(x_train, y_train), (x_test, y_test) = K.datasets.cifar10.load_data()\n",
    "#x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32') \n",
    "y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2)\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_test)\n",
    "\n",
    "\n",
    "print((x_train.shape,y_train.shape))\n",
    "#print((x_val.shape,y_val.shape))\n",
    "print((x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = 10\n",
    "model_short_name = 'resnet18'\n",
    "framework = 'keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.094440221786499\n"
     ]
    }
   ],
   "source": [
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "#model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "#model_name = 'tf_alexnet_cifar10_2021-08-27-17:05:27'\n",
    "model_name = 'tf_resnet18-cifar10_2021-10-29_{}'.format(training_id)\n",
    "model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "size0\n",
    "\n",
    "print(t_elapsed_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.5.0 detected. Last version known to be fully compatible is 2.3.1 .\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import coremltools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/onnx/'\n",
    "coreml_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/coremltools/'\n",
    "error_path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/errors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(y_test, test_predict):\n",
    "    correct_ = np.sum(y_test == test_predict)\n",
    "    accuracy  = correct_*100./np.sum(y_test == y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, y, data_writer_run, batch_size):\n",
    "    \n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "    print(\"converting for batch: \", i)\n",
    "    \n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    ### Original Model\n",
    "    since_1 = time.time()\n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    with tf.device('/cpu:0'): \n",
    "        k_predict = model.predict(x)\n",
    "    inference_time_original = time.time() - since_1\n",
    "    y0 = to_categorical(np.argmax(k_predict, 1), num_classes = 10)\n",
    "    correct_original = np.sum(y0 == y)\n",
    "    accuracy_original = model_scores(y, y0)\n",
    "    # ONNX Model\n",
    "    \n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    since_1 = time.time()\n",
    "    onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    load_time_onnx = time.time() - since_1\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    inference_time_onnx = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    \n",
    "    ####### Mis-classification ONNX ######################################\n",
    "    y2 = to_categorical(np.argmax(ort_outs[0], 1), num_classes = 10)\n",
    "    correct_onnx = np.sum(y2 == y)\n",
    "    accuracy_onnx = model_scores(y, y2)\n",
    "    miss_perc_val_original_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(k_predict, ort_outs[0])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    encoded_miss_perc_val_original_onnx = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                encoded_miss_perc_val_original_onnx = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    miss_perc_val_test_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    ####### End of mis-classification ONNX ###################################### \n",
    "    \n",
    "    \n",
    "    ## CoreML\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    #spec = coreml_model.get_spec()\n",
    "    #coreml_model = coremltools.models.MLModel(spec)\n",
    "    split_ = str(coreml_model.get_spec().description.input[0]).split('\\n')\n",
    "    name_1 = split_[0].replace('name: \"', '')\n",
    "    name_1 = name_1.replace('\"', '')\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    output_dict_test = coreml_model.predict({name_1:x})\n",
    "    inference_time_coreml = time.time() - since_1\n",
    "    ####### Mis-classification coreML ######################################\n",
    "    y3 = to_categorical(np.argmax(output_dict_test['Identity'], 1), num_classes = 10)\n",
    "    correct_coreml = np.sum(y3 == y)\n",
    "    accuracy_coreml = model_scores(y, y3)\n",
    "    \n",
    "    #print(correct_original, correct_coreml, correct_onnx, np.sum(y == y))\n",
    "    ## Part 1\n",
    "    \n",
    "    miss_perc_val_original_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(k_predict, output_dict_test['Identity'])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    ####### Part 2\n",
    "    #print('default-shape: ',k_predict.shape, 'onnx-shape: ',ort_outs[0].shape, 'coreml-shape: ',output_dict_test['Identity'].shape)\n",
    "    miss_perc_val_original_coreml = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_coreml = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    miss_perc_val_test_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    ####### End of mis-classification coreML ######################################\n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, batch_size, i,'onnx',t_elapsed_0, inference_time_original, load_time_onnx, \n",
    "                          inference_time_onnx,  miss_perc_val_original_runtime,'',  encoded_miss_perc_val_original_onnx, miss_perc_val_test_runtime, '', accuracy_original, accuracy_onnx])\n",
    "    \n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, batch_size, i,'coremltools',t_elapsed_0, inference_time_original, load_time_coreml, \n",
    "                          inference_time_coreml,  miss_perc_val_original_runtime2,'',  miss_perc_val_original_coreml, miss_perc_val_test_runtime2, '', accuracy_original,accuracy_coreml])\n",
    "    \n",
    "    #return correct_original,correct_onnx,correct_coreml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lets_convert(data, x, y, data_writer_run, batch_size): # for cifar10 etc\n",
    "    since = time.time()\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in data.flow(x, y, batch_size=batch_size):\n",
    "        to_onnx(batches, x_batch, y_batch, data_writer_run, batch_size)\n",
    "        batches += 1\n",
    "        if batches == 50:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  128\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  9\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  10\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  11\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  12\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  13\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  14\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  15\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  16\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  17\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  18\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  19\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  20\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  21\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  22\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  23\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  24\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  25\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  26\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  27\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  28\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  29\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  30\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  31\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  32\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  33\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  34\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  35\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  36\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  37\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  38\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  39\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  40\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  41\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  42\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  43\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  44\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  45\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  46\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  47\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  48\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  49\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 39s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "if not os.path.exists(error_path+framework+\"/{}\".format(model_short_name)):\n",
    "        Path(error_path+framework+\"/{}\".format(model_short_name)).mkdir(parents=True, exist_ok=True)\n",
    "data_file_run = open(error_path+framework+\"/{}/runtime_miss-classification_{}.csv\".format(model_short_name,model_name), mode='w', newline='',\n",
    "                                  encoding='utf-8')\n",
    "data_writer_run = csv.writer(data_file_run, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "data_writer_run.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'round','runtime','original_load_time', 'original_infererence_time', 'runtime_load_time', \n",
    "                          'runtime_inference_time',  'miss_classified_original_runtime_percentage','',  'encoded_miss_classified_original_runtime_percentage','encoded_miss_classified_original_test_runtime_percentage', '', 'accuracy_original', 'accuracy_runtime'])\n",
    "\n",
    "for batch_size in [128]:\n",
    "    print(\"################ Batch size: \", batch_size)\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2)\n",
    "\n",
    "    datagen.fit(x_test)\n",
    "    _lets_convert(datagen,x_test, y_test,  data_writer_run, batch_size)\n",
    "    #data_writer_acc.writerow([model_short_name,framework, training_id, model_name, batch_size, correct_original, correct_onnx, correct_coreml])\n",
    "data_file_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71945464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_short_name = 'vgg'\n",
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "#model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "#model_name = 'tf_alexnet_cifar10_2021-08-27-17:05:27'\n",
    "model_name = 'tf_vgg-cifar10_2021-10-29_{}'.format(training_id)\n",
    "model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "size0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
