{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/'\n",
    "path_output = '/Volumes/Cisco/Summer2022/onnx-exchange/analysis/conversion/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['lenet5', 'resnet18', 'vgg', 'lstm', 'gru']\n",
    "frameworks = ['keras', 'pytorch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(path_output+'metrics_miss-classifications2.csv', mode='w', newline='', encoding='utf-8')\n",
    "data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#data_writer.writerow(['model','Epoch', 'Training_accuracy', 'Validation_Accuracy', \"Training_Loss\", 'Validation_Loss'])\n",
    "data_writer.writerow(['framework','model', 'loadtime_before', 'loadtime_onnx', 'loadtime_coreml', 'miss_classification_onnx', 'miss_classification_coreml', 'inferencetime_before', 'inferencetime_onnx', 'inferencetime_coreml', '', 'loadtime_before_std', 'loadtime_onnx_std', 'loadtime_coreml_std', 'miss_classification_onnx_std', 'miss_classification_coreml_std', 'inferencetime_before_std', 'inferencetime_onnx_std', 'inferencetime_coreml_std', 'miss_test_onnx', 'miss_test_coreml'])\n",
    "\n",
    "for frame_ in frameworks:\n",
    "    for model in models: \n",
    "        data_original_load_time = []\n",
    "        data_original_infererence_time = []\n",
    "        data_runtime_conversion_time = {}\n",
    "        data_runtime_load_time = {}\n",
    "        data_runtime_inference_time = {}\n",
    "        data_encoded_miss_classified_original_test_runtime_percentage = {}\n",
    "        data_encoded_miss_classified_original_runtime_percentage = {}\n",
    "\n",
    "        if os.path.exists(path+'errors/{}/{}'.format(frame_,model)):\n",
    "            list_files = [x for x in os.listdir(path+'errors/{}/{}'.format(frame_,model)) if 'runtime_miss-classification' in x]\n",
    "            #print('yes', list_files)\n",
    "            for i in range(1, 11):\n",
    "                for file_ in list_files:\n",
    "                    df = pd.read_csv(path+'errors/{}/{}/{}'.format(frame_,model, file_))\n",
    "                    #print(df.columns)\n",
    "                    batch_size = df.batch_size.values.tolist()\n",
    "                    converter = df.runtime.values.tolist()\n",
    "                    original_load_time = df.original_load_time.values.tolist()\n",
    "                    original_infererence_time = df.original_infererence_time.values.tolist()\n",
    "                    #runtime_conversion_time = df.runtime_conversion_time.values.tolist()\n",
    "                    #runtime_saving_time = df.runtime_saving_time.values.tolist()\n",
    "                    runtime_load_time = df.runtime_load_time.values.tolist()\n",
    "                    runtime_inference_time = df.runtime_inference_time.values.tolist()\n",
    "                    encoded_miss_classified_original_runtime_percentage = df.encoded_miss_classified_original_runtime_percentage.values.tolist()\n",
    "                    encoded_miss_classified_original_test_runtime_percentage = df.encoded_miss_classified_original_test_runtime_percentage.values.tolist()\n",
    "                    \n",
    "                    for j in range(len(converter)):\n",
    "                        if batch_size[j] == 128:\n",
    "                            data_original_load_time.append(original_load_time[j])\n",
    "                            data_original_infererence_time.append(original_infererence_time[j])\n",
    "                            if converter[j] in data_runtime_load_time.keys():\n",
    "                                #data_inference_before[converter[j]].append(original_infererence_time[j])\n",
    "                                #data_runtime_conversion_time[converter[j]].append(runtime_conversion_time[j])\n",
    "                                data_runtime_load_time[converter[j]].append(runtime_load_time[j])\n",
    "                                data_runtime_inference_time[converter[j]].append(runtime_inference_time[j])\n",
    "\n",
    "                                #data_memory_before[converter[j]].append(original_size[j])\n",
    "                                data_encoded_miss_classified_original_test_runtime_percentage[converter[j]] += float(str(encoded_miss_classified_original_runtime_percentage[j]).replace('%', ''))\n",
    "                                data_encoded_miss_classified_original_runtime_percentage[converter[j]] += float(str(encoded_miss_classified_original_test_runtime_percentage[j]).replace('%', ''))\n",
    "                            else:\n",
    "                                #data_inference_before[converter[j]] = [original_infererence_time[j]]\n",
    "                                #data_runtime_conversion_time[converter[j]] = [runtime_conversion_time[j]]\n",
    "                                \n",
    "                                data_runtime_load_time[converter[j]] = [runtime_load_time[j]]\n",
    "                                data_runtime_inference_time[converter[j]] = [runtime_inference_time[j]]\n",
    "\n",
    "                                #data_memory_before[converter[j]] = [original_size[j]]\n",
    "                                data_encoded_miss_classified_original_test_runtime_percentage[converter[j]] = float(str(encoded_miss_classified_original_runtime_percentage[j]).replace('%', ''))\n",
    "                                data_encoded_miss_classified_original_runtime_percentage[converter[j]] = float(str(encoded_miss_classified_original_test_runtime_percentage[j]).replace('%', ''))\n",
    "            #for key, val in data_inference_before.items()\n",
    "            total_len = len(data_runtime_load_time['onnx'])\n",
    "            #data_writer.writerow([frame_,model, np.mean(data_inference_before), np.mean(data_inference_after['onnx']), np.mean(data_inference_after['coremltools']), np.mean(data_memory_before), np.mean(data_memory_after['onnx']), np.mean(data_memory_after['coremltools'])])\n",
    "            data_writer.writerow([frame_,model, '{:.2e}'.format(np.mean(data_original_load_time)), '{:.2e}'.format(np.mean(data_runtime_load_time['onnx'])), '{:.2e}'.format(np.mean(data_runtime_load_time['coremltools'])), '{:.2f}%'.format(np.sum(data_encoded_miss_classified_original_runtime_percentage['onnx'])/(100*total_len)), '{:.2f}%'.format(np.sum(data_encoded_miss_classified_original_runtime_percentage['coremltools'])/(100*total_len)), '{:.2e}'.format(np.mean(original_infererence_time)), '{:.2e}'.format(np.mean(data_runtime_inference_time['onnx'])), '{:.2e}'.format(np.mean(data_runtime_inference_time['coremltools'])), '',\n",
    "                                 '{:.2e}'.format(np.std(data_original_load_time)), '{:.2e}'.format(np.std(data_runtime_load_time['onnx'])), '{:.2e}'.format(np.std(data_runtime_load_time['coremltools'])), '{:.2f}%'.format(np.std(data_encoded_miss_classified_original_runtime_percentage['onnx'])), '{:.2f}%'.format(np.std(data_encoded_miss_classified_original_runtime_percentage['coremltools'])), '{:.2e}'.format(np.std(original_infererence_time)), '{:.2e}'.format(np.std(data_runtime_inference_time['onnx'])), '{:.2e}'.format(np.std(data_runtime_inference_time['coremltools'])), \n",
    "                                 '{:.2f}%'.format(np.sum(data_encoded_miss_classified_original_runtime_percentage['onnx'])/(100*total_len)), '{:.2f}%'.format(np.sum(data_encoded_miss_classified_original_runtime_percentage['coremltools'])/(100*total_len))])\n",
    "\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5649452209472656,\n",
       " 2.5649452209472656,\n",
       " 2.65227484703064,\n",
       " 2.65227484703064,\n",
       " 2.712142944335937,\n",
       " 2.712142944335937,\n",
       " 2.676571846008301,\n",
       " 2.676571846008301,\n",
       " 2.410376071929932,\n",
       " 2.410376071929932,\n",
       " 2.440882921218872,\n",
       " 2.440882921218872,\n",
       " 2.424577236175537,\n",
       " 2.424577236175537,\n",
       " 2.788219928741455,\n",
       " 2.788219928741455,\n",
       " 2.84631896018982,\n",
       " 2.84631896018982,\n",
       " 2.8421571254730225,\n",
       " 2.8421571254730225,\n",
       " 2.8847968578338623,\n",
       " 2.8847968578338623]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_infererence_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (23,) (25,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-41a45965661d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_runtime_load_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m#data_writer.writerow([frame_,model, np.mean(data_inference_before), np.mean(data_inference_after['onnx']), np.mean(data_inference_after['coremltools']), np.mean(data_memory_before), np.mean(data_memory_after['onnx']), np.mean(data_memory_after['coremltools'])])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             data_writer.writerow([frame_,model, '{:.2e}'.format(np.mean(data_original_load_time_2)), '{:.2e}'.format(np.mean(data_runtime_load_time_2['onnx'])), '{:.2e}'.format(np.mean(data_runtime_load_time_2['coremltools'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['onnx'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['coremltools'])), '{:.2e}'.format(np.mean(data_original_infererence_time_2)), '{:.2e}'.format(np.mean(data_runtime_inference_time_2['onnx'])), '{:.2e}'.format(np.mean(data_runtime_inference_time_2['coremltools'])), '',\n\u001b[0m\u001b[1;32m     91\u001b[0m                                  \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_original_load_time_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_runtime_load_time_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_runtime_load_time_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coremltools'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_encoded_miss_classified_original_runtime_percentage_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_encoded_miss_classified_original_runtime_percentage_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coremltools'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_infererence_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_runtime_inference_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.2e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_runtime_inference_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coremltools'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                  '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['onnx'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['coremltools']))])\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3373\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (23,) (25,) "
     ]
    }
   ],
   "source": [
    "data_file = open(path_output+'metrics_miss-classifications2.csv', mode='w', newline='', encoding='utf-8')\n",
    "data_writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#data_writer.writerow(['model','Epoch', 'Training_accuracy', 'Validation_Accuracy', \"Training_Loss\", 'Validation_Loss'])\n",
    "data_writer.writerow(['framework','model', 'loadtime_before', 'loadtime_onnx', 'loadtime_coreml', 'miss_classification_onnx', 'miss_classification_coreml', 'inferencetime_before', 'inferencetime_onnx', 'inferencetime_coreml', '', 'loadtime_before_std', 'loadtime_onnx_std', 'loadtime_coreml_std', 'miss_classification_onnx_std', 'miss_classification_coreml_std', 'inferencetime_before_std', 'inferencetime_onnx_std', 'inferencetime_coreml_std', 'miss_test_onnx', 'miss_test_coreml'])\n",
    "\n",
    "for frame_ in frameworks:\n",
    "    for model in models: \n",
    "        data_original_load_time_2 = []\n",
    "        data_original_infererence_time_2 = []\n",
    "        data_runtime_conversion_time_2 = {}\n",
    "        data_runtime_load_time_2 = {}\n",
    "        data_runtime_inference_time_2 = {}\n",
    "        data_encoded_miss_classified_original_test_runtime_percentage_2 = {}\n",
    "        data_encoded_miss_classified_original_runtime_percentage_2 = {}\n",
    "\n",
    "        if os.path.exists(path+'errors/{}/{}'.format(frame_,model)):\n",
    "            list_files = [x for x in os.listdir(path+'errors/{}/{}'.format(frame_,model)) if 'runtime_miss-classification' in x]\n",
    "            #print('yes', list_files)\n",
    "            for i in range(1, 11):\n",
    "                for file_ in list_files:\n",
    "                    df = pd.read_csv(path+'errors/{}/{}/{}'.format(frame_,model, file_))\n",
    "                    #print(df.columns)\n",
    "                    batch_size = df.batch_size.values.tolist()\n",
    "                    converter = df.runtime.values.tolist()\n",
    "                    original_load_time = df.original_load_time.values.tolist()\n",
    "                    original_infererence_time = df.original_infererence_time.values.tolist()\n",
    "                    #runtime_conversion_time = df.runtime_conversion_time.values.tolist()\n",
    "                    #runtime_saving_time = df.runtime_saving_time.values.tolist()\n",
    "                    runtime_load_time = df.runtime_load_time.values.tolist()\n",
    "                    runtime_inference_time = df.runtime_inference_time.values.tolist()\n",
    "                    encoded_miss_classified_original_runtime_percentage = df.encoded_miss_classified_original_runtime_percentage.values.tolist()\n",
    "                    encoded_miss_classified_original_test_runtime_percentage = df.encoded_miss_classified_original_test_runtime_percentage.values.tolist()\n",
    "                    \n",
    "                    data_original_load_time = []\n",
    "                    data_original_infererence_time = []\n",
    "                    data_runtime_conversion_time = {}\n",
    "                    data_runtime_load_time = {}\n",
    "                    data_runtime_inference_time = {}\n",
    "                    data_encoded_miss_classified_original_test_runtime_percentage = {}\n",
    "                    data_encoded_miss_classified_original_runtime_percentage = {}\n",
    "        \n",
    "                    for j in range(len(converter)):\n",
    "                        if batch_size[j] == 128:\n",
    "                            data_original_load_time.append(original_load_time[j])\n",
    "                            data_original_infererence_time.append(original_infererence_time[j])\n",
    "                            if converter[j] in data_runtime_load_time.keys():\n",
    "                                #data_inference_before[converter[j]].append(original_infererence_time[j])\n",
    "                                #data_runtime_conversion_time[converter[j]].append(runtime_conversion_time[j])\n",
    "                                data_runtime_load_time[converter[j]].append(runtime_load_time[j])\n",
    "                                data_runtime_inference_time[converter[j]].append(runtime_inference_time[j])\n",
    "\n",
    "                                #data_memory_before[converter[j]].append(original_size[j])\n",
    "                                data_encoded_miss_classified_original_test_runtime_percentage[converter[j]] += float(str(encoded_miss_classified_original_runtime_percentage[j]).replace('%', ''))\n",
    "                                data_encoded_miss_classified_original_runtime_percentage[converter[j]] += float(str(encoded_miss_classified_original_test_runtime_percentage[j]).replace('%', ''))\n",
    "                            else:\n",
    "                                #data_inference_before[converter[j]] = [original_infererence_time[j]]\n",
    "                                #data_runtime_conversion_time[converter[j]] = [runtime_conversion_time[j]]\n",
    "                                \n",
    "                                data_runtime_load_time[converter[j]] = [runtime_load_time[j]]\n",
    "                                data_runtime_inference_time[converter[j]] = [runtime_inference_time[j]]\n",
    "\n",
    "                                #data_memory_before[converter[j]] = [original_size[j]]\n",
    "                                data_encoded_miss_classified_original_test_runtime_percentage[converter[j]] = float(str(encoded_miss_classified_original_runtime_percentage[j]).replace('%', ''))\n",
    "                                data_encoded_miss_classified_original_runtime_percentage[converter[j]] = float(str(encoded_miss_classified_original_test_runtime_percentage[j]).replace('%', ''))\n",
    "                \n",
    "                    keys_ = ['onnx', 'coremltools']\n",
    "                    for key_ in keys_:\n",
    "                        if key_ in data_runtime_load_time_2.keys():\n",
    "                            len_ = data_runtime_load_time[key_]\n",
    "                            #print(data_runtime_load_time)\n",
    "                            data_runtime_load_time_2[key_].append(np.mean(data_runtime_load_time[key_]))\n",
    "                            data_runtime_inference_time_2[key_].append(np.mean(data_runtime_inference_time[key_]))\n",
    "                            #data_original_load_time_2[key_].append(np.mean(data_runtime_load_time[key_]))\n",
    "                            data_encoded_miss_classified_original_test_runtime_percentage_2[key_].append(np.sum(data_encoded_miss_classified_original_test_runtime_percentage[key_])/len_)\n",
    "                            data_encoded_miss_classified_original_runtime_percentage_2[key_].append(np.sum(data_encoded_miss_classified_original_runtime_percentage[key_])/len_)\n",
    "                            #data_original_load_time_2.append(np.mean(data_original_load_time))\n",
    "                            #data_original_infererence_time_2.append(np.mean(data_original_infererence_time))\n",
    "                        else:\n",
    "                            len_ = data_runtime_load_time[key_]\n",
    "                            data_runtime_load_time_2[key_] = [np.mean(data_runtime_load_time[key_])]\n",
    "                            data_runtime_inference_time_2[key_] = [np.mean(data_runtime_inference_time[key_])]\n",
    "                            #data_original_load_time_2[key_].append(np.mean(data_runtime_load_time[key_]))\n",
    "                            data_encoded_miss_classified_original_test_runtime_percentage_2[key_] = [np.sum(data_encoded_miss_classified_original_test_runtime_percentage[key_])/len_]\n",
    "                            data_encoded_miss_classified_original_runtime_percentage_2[key_] = [np.sum(data_encoded_miss_classified_original_runtime_percentage[key_])/len_]\n",
    "\n",
    "                    \n",
    "            #for key, val in data_inference_before.items()\n",
    "            total_len = len(data_runtime_load_time['onnx'])\n",
    "            #data_writer.writerow([frame_,model, np.mean(data_inference_before), np.mean(data_inference_after['onnx']), np.mean(data_inference_after['coremltools']), np.mean(data_memory_before), np.mean(data_memory_after['onnx']), np.mean(data_memory_after['coremltools'])])\n",
    "            data_writer.writerow([frame_,model, '{:.2e}'.format(np.mean(data_original_load_time_2)), '{:.2e}'.format(np.mean(data_runtime_load_time_2['onnx'])), '{:.2e}'.format(np.mean(data_runtime_load_time_2['coremltools'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['onnx'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['coremltools'])), '{:.2e}'.format(np.mean(data_original_infererence_time_2)), '{:.2e}'.format(np.mean(data_runtime_inference_time_2['onnx'])), '{:.2e}'.format(np.mean(data_runtime_inference_time_2['coremltools'])), '',\n",
    "                                 '{:.2e}'.format(np.std(data_original_load_time_2)), '{:.2e}'.format(np.std(data_runtime_load_time_2['onnx'])), '{:.2e}'.format(np.std(data_runtime_load_time_2['coremltools'])), '{:.2f}%'.format(np.std(data_encoded_miss_classified_original_runtime_percentage_2['onnx'])), '{:.2f}%'.format(np.std(data_encoded_miss_classified_original_runtime_percentage_2['coremltools'])), '{:.2e}'.format(np.std(original_infererence_time)), '{:.2e}'.format(np.std(data_runtime_inference_time['onnx'])), '{:.2e}'.format(np.std(data_runtime_inference_time['coremltools'])), \n",
    "                                 '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['onnx'])), '{:.2f}%'.format(np.mean(data_encoded_miss_classified_original_runtime_percentage_2['coremltools']))])\n",
    "\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
