{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import tf2onnx\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (60000, 28, 28, 1)\n",
      "60000 training samples\n",
      "10000 test samples\n",
      "One-hot encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500 # Number of images processed at once\n",
    "nb_classes = 10  # 10 Digits from 0 to 9\n",
    "\n",
    "# Dimensionen of the input images (28x28 pixel)\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Load image data with labels, split into test and training set \n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape images in 4D tensor (N images, 28 rows, 28 columns, 1 channel) \n",
    "# rescale pixels range from [0, 255] to [0, 1]\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert digit labels (0-9) in one-hot encoded binary vectors. \n",
    "# These correspond to the training/test labels at the output of the net. \n",
    "Y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "print(\"One-hot encoding: {}\".format(Y_train[0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() ## for Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    " \n",
    "    def __init__(self, X_data , y_data, batch_size, dim, n_classes,\n",
    "                 to_fit, shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X_data = X_data\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.X_data))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        \n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        \n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes)/self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:\n",
    "            (index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        \n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indexes = np.arange(len(self.X_data))\n",
    "        \n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "               \n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            X[i,] = self.X_data[ID]\n",
    "            \n",
    "            # Normalize data\n",
    "            X = (X/255).astype('float32')\n",
    "            \n",
    "        return X[:,:,:, np.newaxis]\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        \n",
    "        y = np.empty(self.batch_size)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            y[i] = self.y_data[ID]\n",
    "            \n",
    "        return keras.utils.to_categorical(y,num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "nb_classes = 10\n",
    "input_shape = (28, 28) #Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = 4\n",
    "model_short_name = 'Lenet5'\n",
    "framework = 'keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/Keras/lenet5/'\n",
    "since_0 = time.time()\n",
    "#model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "#model_name = 'tf_alexnet_cifar10_2021-08-27-17:05:27'\n",
    "model_name = 'tf_Lenet5_mnist_2021-10-27_{}'.format(training_id)\n",
    "model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "size0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.5.0 detected. Last version known to be fully compatible is 2.3.1 .\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import coremltools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/onnx/'\n",
    "coreml_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/coremltools/'\n",
    "error_path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def model_scores(y_test, test_predict):\n",
    "    total_ = np.sum(y_test == y_test)\n",
    "    correct_ = np.sum(y_test == test_predict)\n",
    "    accuracy2  = correct_ #*100./total_\n",
    "    return accuracy2 #, precision2, recall2, f12\n",
    "def get_miss_classification(y1, y2):\n",
    "    miss_perc_val_original_coreml = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y1[0], y2[0])\n",
    "    except Exception as e:\n",
    "        miss_perc_val_original_coreml = 1\n",
    "    return miss_perc_val_original_coreml #, precision2, recall2, f12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, y, data_writer_run, batch_size):\n",
    "    \n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "    #print(\"converting for batch: \", i)\n",
    "    \n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    ### Original Model\n",
    "    since_1 = time.time()\n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    with tf.device('/cpu:0'): \n",
    "        k_predict = model.predict(x)\n",
    "    inference_time_original = time.time() - since_1\n",
    "    y0 = to_categorical(np.argmax(k_predict, 1), num_classes = 10)\n",
    "    correct_original = np.sum(y0 == y)\n",
    "    accuracy_original = model_scores(y, y0)\n",
    "    miss_test_original = get_miss_classification(y, y0)\n",
    "    # ONNX Model\n",
    "    \n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    #since_1 = time.time()\n",
    "    #onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    #load_time_onnx = time.time() - since_1\n",
    "    #onnx.checker.check_model(onnx_model)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    #ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    inference_time_onnx = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    #print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    \n",
    "    ####### Mis-classification ONNX ######################################\n",
    "    y2 = to_categorical(np.argmax(ort_outs[0], 1), num_classes = 10)\n",
    "    correct_onnx = np.sum(y2 == y)\n",
    "    accuracy_onnx = model_scores(y, y2)\n",
    "    \n",
    "    miss_original_onnx = get_miss_classification(y0, y2)\n",
    "    miss_test_onnx = get_miss_classification(y, y2)\n",
    "    ####### End of mis-classification ONNX ###################################### \n",
    "    \n",
    "    \n",
    "    ## CoreML\n",
    "    \n",
    "    #since_1 = time.time()\n",
    "    #coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    #load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    #spec = coreml_model.get_spec()\n",
    "    #coreml_model = coremltools.models.MLModel(spec)\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    output_dict_test = coreml_model.predict({'conv2d_input':x})\n",
    "    inference_time_coreml = time.time() - since_1\n",
    "    ####### Mis-classification coreML ######################################\n",
    "    y3 = to_categorical(np.argmax(output_dict_test['Identity'], 1), num_classes = 10)\n",
    "    correct_coreml = np.sum(y3 == y)\n",
    "    accuracy_coreml = model_scores(y, y3)\n",
    "    \n",
    "    #print(correct_original, correct_coreml, correct_onnx, np.sum(y == y))\n",
    "    ## Part 1\n",
    "    ####### Part 2\n",
    "    #print('default-shape: ',k_predict.shape, 'onnx-shape: ',ort_outs[0].shape, 'coreml-shape: ',output_dict_test['Identity'].shape)\n",
    "    miss_original_coreml = get_miss_classification(y0, y3)\n",
    "    miss_test_coreml = get_miss_classification(y, y3)\n",
    "    ####### End of mis-classification coreML ######################################\n",
    "    #return correct_original,correct_onnx,correct_coreml\n",
    "    list_val = [accuracy_original, accuracy_onnx, accuracy_coreml, miss_test_original, miss_original_onnx, miss_test_onnx, miss_original_coreml, miss_test_coreml,'', inference_time_original, inference_time_onnx, inference_time_coreml]\n",
    "    return list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lets_convert(data, data_writer_run, batch_size):\n",
    "    since = time.time()\n",
    "    accuracy_original = 0.0\n",
    "    accuracy_onnx = 0.0\n",
    "    accuracy_coreml = 0.0\n",
    "    \n",
    "    miss_test_original = 0.0\n",
    "    miss_original_onnx = 0\n",
    "    miss_test_onnx = 0\n",
    "    miss_original_coreml = 0\n",
    "    miss_test_coreml = 0\n",
    "    \n",
    "    inference_time_original = 0.0\n",
    "    inference_time_onnx = 0.0\n",
    "    inference_time_coreml = 0.0\n",
    "    \n",
    "    total_ = 0.0\n",
    "    total_datasets = 0\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        #torch.cuda.empty_cache()\n",
    "        #images = images.cuda()\n",
    "        #for j in range(len(images)):\n",
    "        list_val = to_onnx(i, images,labels, data_writer_run, batch_size)\n",
    "        \n",
    "        if i%50 == 0:\n",
    "            print(i, list_val)\n",
    "        accuracy_original += list_val[0]\n",
    "        accuracy_onnx += list_val[1]\n",
    "        accuracy_coreml += list_val[2]\n",
    "        miss_test_original += list_val[3]\n",
    "        miss_original_onnx += list_val[4]\n",
    "        miss_test_onnx += list_val[5]\n",
    "        miss_original_coreml += list_val[6]\n",
    "        miss_test_coreml += list_val[7]\n",
    "        \n",
    "        inference_time_original += list_val[9]\n",
    "        inference_time_onnx += list_val[10]\n",
    "        inference_time_coreml += list_val[11]\n",
    "        total_ += np.sum(labels == labels)\n",
    "        total_datasets += labels.shape[0]\n",
    "        if i == 1000:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60) )\n",
    "    #print(correct_original, correct_onnx, correct_coreml, total, correct_original*100./total, precision2, recall2, f1)\n",
    "    #return correct_original*100./total, correct_onnx*100./total, correct_coreml*100./total\n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, batch_size, total_,total_datasets, round(accuracy_original*100/total_,2), round(accuracy_onnx*100/total_,2), round(accuracy_coreml*100/total_,2), round(miss_test_original*100/total_datasets,2), round(miss_original_onnx*100/total_datasets,2), round(miss_test_onnx*100/total_datasets,2), round(miss_original_coreml*100/total_datasets,2), round(miss_test_coreml*100/total_datasets,2),'', inference_time_original/total_datasets, inference_time_onnx/total_datasets, inference_time_coreml/total_datasets, '',\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_original/total_datasets) // 60, (inference_time_original/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_onnx/total_datasets) // 60, (inference_time_onnx/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_coreml/total_datasets) // 60, (inference_time_coreml/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  1  traiining id:  5\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.20145702362060547, 0.00039696693420410156, 0.005462169647216797]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0712440013885498, 0.00028896331787109375, 0.002574920654296875]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06877803802490234, 0.0005848407745361328, 0.0030341148376464844]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06469392776489258, 0.0003046989440917969, 0.003962278366088867]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05756807327270508, 0.0004031658172607422, 0.004266023635864258]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.052102088928222656, 0.0002961158752441406, 0.00442814826965332]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0569148063659668, 0.0002999305725097656, 0.0038938522338867188]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07092404365539551, 0.0002658367156982422, 0.0024492740631103516]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.20476698875427246, 0.0005071163177490234, 0.005456209182739258]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05648207664489746, 0.0003020763397216797, 0.0039441585540771484]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05974102020263672, 0.0002689361572265625, 0.003762960433959961]\n",
      "550 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06356000900268555, 0.00030922889709472656, 0.004141807556152344]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1104428768157959, 0.0005221366882324219, 0.005372285842895508]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.09019804000854492, 0.00040793418884277344, 0.004206180572509766]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05331683158874512, 0.000308990478515625, 0.003990888595581055]\n",
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08919215202331543, 0.0003299713134765625, 0.004119873046875]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.09151411056518555, 0.0002918243408203125, 0.006970882415771484]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06540703773498535, 0.00020122528076171875, 0.008888006210327148]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11011981964111328, 0.0002808570861816406, 0.0065648555755615234]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1141672134399414, 0.0002551078796386719, 0.0025582313537597656]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.13758492469787598, 0.0003540515899658203, 0.013076066970825195]\n",
      "Conversion complete in 1m 33s\n",
      "################ Batch size:  1  traiining id:  6\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.4004349708557129, 0.0005030632019042969, 0.004019021987915039]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07953596115112305, 0.00035309791564941406, 0.014261960983276367]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08642816543579102, 0.0002617835998535156, 0.002653360366821289]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1038370132446289, 0.0020890235900878906, 0.0033659934997558594]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11263179779052734, 0.0002968311309814453, 0.009454727172851562]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08585405349731445, 0.0002961158752441406, 0.0038480758666992188]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10734796524047852, 0.00023603439331054688, 0.0021970272064208984]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10403919219970703, 0.0003161430358886719, 0.006497621536254883]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10642695426940918, 0.0002651214599609375, 0.002474069595336914]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11754918098449707, 0.0005819797515869141, 0.003114938735961914]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.13084912300109863, 0.00027489662170410156, 0.0030760765075683594]\n",
      "550 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06641912460327148, 0.0002410411834716797, 0.002371072769165039]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06164193153381348, 0.0002696514129638672, 0.004128932952880859]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.12983989715576172, 0.0003273487091064453, 0.005259990692138672]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06595087051391602, 0.0002491474151611328, 0.002287149429321289]\n",
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07059502601623535, 0.00029587745666503906, 0.00683283805847168]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.16979098320007324, 0.0003058910369873047, 0.0032072067260742188]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11942195892333984, 0.0002751350402832031, 0.00586390495300293]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06663298606872559, 0.0002980232238769531, 0.002825021743774414]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06048274040222168, 0.00032210350036621094, 0.002893209457397461]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.060037851333618164, 0.00030493736267089844, 0.004007101058959961]\n",
      "Conversion complete in 1m 35s\n",
      "################ Batch size:  1  traiining id:  7\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.2155592441558838, 0.0003559589385986328, 0.002942800521850586]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07501602172851562, 0.00021409988403320312, 0.003782987594604492]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08820891380310059, 0.0003829002380371094, 0.008365631103515625]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0570368766784668, 0.0002999305725097656, 0.003896951675415039]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11016035079956055, 0.00032401084899902344, 0.0045130252838134766]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08754181861877441, 0.0003008842468261719, 0.0041849613189697266]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05845284461975098, 0.00037598609924316406, 0.0043468475341796875]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.059607744216918945, 0.00023508071899414062, 0.0023779869079589844]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08052802085876465, 0.00020813941955566406, 0.006790876388549805]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07841610908508301, 0.00029921531677246094, 0.003017902374267578]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06618499755859375, 0.0003063678741455078, 0.003916025161743164]\n",
      "550 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08646297454833984, 0.00028705596923828125, 0.007776021957397461]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08139491081237793, 0.00032806396484375, 0.0027539730072021484]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08621692657470703, 0.00028896331787109375, 0.0028579235076904297]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10376334190368652, 0.0002923011779785156, 0.00903177261352539]\n",
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07103991508483887, 0.0002448558807373047, 0.004202842712402344]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08678507804870605, 0.00030493736267089844, 0.0031249523162841797]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.09116697311401367, 0.000286102294921875, 0.0034668445587158203]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10371875762939453, 0.0003039836883544922, 0.00445103645324707]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07725381851196289, 0.0002779960632324219, 0.0025680065155029297]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.11625194549560547, 0.00026488304138183594, 0.0024247169494628906]\n",
      "Conversion complete in 1m 32s\n",
      "################ Batch size:  1  traiining id:  8\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.2532689571380615, 0.0004191398620605469, 0.00455927848815918]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08364701271057129, 0.0004341602325439453, 0.017004013061523438]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.12709975242614746, 0.0003859996795654297, 0.0028150081634521484]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07903504371643066, 0.00033593177795410156, 0.002580881118774414]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.16938996315002441, 0.0002980232238769531, 0.003480195999145508]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06746101379394531, 0.0003268718719482422, 0.004300117492675781]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05924487113952637, 0.00047206878662109375, 0.004063129425048828]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08750224113464355, 0.00028514862060546875, 0.002997159957885742]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1817929744720459, 0.00039768218994140625, 0.0029401779174804688]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1091010570526123, 0.0003561973571777344, 0.0025348663330078125]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08937907218933105, 0.00030303001403808594, 0.004911899566650391]\n",
      "550 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06837081909179688, 0.0003139972686767578, 0.002808809280395508]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08463001251220703, 0.0003151893615722656, 0.0026731491088867188]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06377696990966797, 0.000293731689453125, 0.0027883052825927734]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.058840036392211914, 0.0004138946533203125, 0.018537044525146484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.255511999130249, 0.0004208087921142578, 0.00435185432434082]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.09183478355407715, 0.00035834312438964844, 0.006600856781005859]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06464624404907227, 0.000308990478515625, 0.002414226531982422]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.12916994094848633, 0.0023469924926757812, 0.004087924957275391]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07454895973205566, 0.0002720355987548828, 0.0027480125427246094]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08181118965148926, 0.0002620220184326172, 0.002454996109008789]\n",
      "Conversion complete in 1m 35s\n",
      "################ Batch size:  1  traiining id:  9\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.21673583984375, 0.0003418922424316406, 0.002930879592895508]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.09177899360656738, 0.00028705596923828125, 0.002663135528564453]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.23146700859069824, 0.0004258155822753906, 0.006048917770385742]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07927083969116211, 0.00030684471130371094, 0.0025298595428466797]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.20249414443969727, 0.0006909370422363281, 0.003065824508666992]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.1022481918334961, 0.00031304359436035156, 0.0029730796813964844]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06148695945739746, 0.0004260540008544922, 0.0037729740142822266]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07006287574768066, 0.00043320655822753906, 0.0026068687438964844]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06473803520202637, 0.00028586387634277344, 0.002694845199584961]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.12888312339782715, 0.0002779960632324219, 0.002580881118774414]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0821840763092041, 0.0002777576446533203, 0.0036699771881103516]\n",
      "550 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0791471004486084, 0.0002779960632324219, 0.002824068069458008]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06924796104431152, 0.00023293495178222656, 0.002329111099243164]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07229900360107422, 0.00028514862060546875, 0.0027608871459960938]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06488394737243652, 0.0004451274871826172, 0.002897024154663086]\n",
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06226801872253418, 0.00046181678771972656, 0.004681825637817383]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06756877899169922, 0.0002951622009277344, 0.0029718875885009766]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07265210151672363, 0.00028896331787109375, 0.003905057907104492]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08662295341491699, 0.0003230571746826172, 0.0025539398193359375]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06498003005981445, 0.0002739429473876953, 0.004065036773681641]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.057008981704711914, 0.000308990478515625, 0.002562999725341797]\n",
      "Conversion complete in 1m 32s\n",
      "################ Batch size:  1  traiining id:  10\n",
      "0 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.14290189743041992, 0.0004329681396484375, 0.005328178405761719]\n",
      "50 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.060913801193237305, 0.0003299713134765625, 0.002730131149291992]\n",
      "100 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0600740909576416, 0.00026607513427734375, 0.003759145736694336]\n",
      "150 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07036328315734863, 0.00031113624572753906, 0.002525806427001953]\n",
      "200 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05771589279174805, 0.00029206275939941406, 0.0038399696350097656]\n",
      "250 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06710386276245117, 0.00028228759765625, 0.002686023712158203]\n",
      "300 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.058114051818847656, 0.0003108978271484375, 0.002714872360229492]\n",
      "350 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06993603706359863, 0.00038909912109375, 0.002542734146118164]\n",
      "400 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0836789608001709, 0.00026679039001464844, 0.0023610591888427734]\n",
      "450 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07081103324890137, 0.00028014183044433594, 0.002610921859741211]\n",
      "500 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.10135102272033691, 0.0002968311309814453, 0.0026140213012695312]\n",
      "550 [8, 8, 8, 1, 0, 1, 0, 1, '', 0.0724329948425293, 0.0002789497375488281, 0.0027866363525390625]\n",
      "600 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.07496929168701172, 0.00021505355834960938, 0.0021390914916992188]\n",
      "650 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06177115440368652, 0.0002968311309814453, 0.011797189712524414]\n",
      "700 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.062333106994628906, 0.00035381317138671875, 0.004930019378662109]\n",
      "750 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06281089782714844, 0.0002608299255371094, 0.0037360191345214844]\n",
      "800 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05933094024658203, 0.000331878662109375, 0.004163026809692383]\n",
      "850 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.05380821228027344, 0.0002949237823486328, 0.0027589797973632812]\n",
      "900 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.06421899795532227, 0.0003330707550048828, 0.0041539669036865234]\n",
      "950 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.0643301010131836, 0.0003001689910888672, 0.00249481201171875]\n",
      "1000 [10, 10, 10, 0, 0, 0, 0, 0, '', 0.08267903327941895, 0.0003452301025390625, 0.0029909610748291016]\n",
      "Conversion complete in 1m 20s\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'letnet5-keras'\n",
    "import pandas as pd \n",
    "if not os.path.exists(error_path+framework):\n",
    "    Path(error_path+framework).mkdir(parents=True, exist_ok=True)\n",
    "#data_file_run = open(error_path+framework+\"/{}/runtime_miss-classification_{}.csv\".format(model_short_name,model_name), mode='w', newline='',\n",
    "#                                  encoding='utf-8')\n",
    "#data_writer_run = csv.writer(data_file_run, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#data_writer_run.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'round','runtime','original_load_time', 'original_infererence_time', 'runtime_load_time', \n",
    "#                          'runtime_inference_time',  'miss_classified_original_runtime_percentage','',  'encoded_miss_classified_original_runtime_percentage','encoded_miss_classified_original_test_runtime_percentage', '', 'accuracy_original', 'accuracy_runtime'])\n",
    "\n",
    "for round_ in [5,6,7,8,9,10]: #[5,6,7,8,9,10]:\n",
    "    training_id = round_\n",
    "    model_short_name = 'Lenet5'\n",
    "    framework = 'keras'\n",
    "    path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/Keras/lenet5/'\n",
    "    since_0 = time.time()\n",
    "    #model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "    #model_name = 'tf_alexnet_cifar10_2021-08-27-17:05:27'\n",
    "    model_name = 'tf_Lenet5_mnist_2021-10-28_{}'.format(training_id)\n",
    "    model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "    t_elapsed_0 = time.time() - since_0\n",
    "    size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "    size0\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    load_time_onnx = time.time() - since_1\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    \n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    load_time_coreml = time.time() - since_1\n",
    "\n",
    "    flag = 0\n",
    "    if not os.path.exists(error_path+framework+\"/runtime_accuracy_{}2.csv\".format(model_short_name)):\n",
    "        data_file_run2 = open(error_path+framework+\"/runtime_accuracy_{}2.csv\".format(model_short_name), mode='w', newline='', encoding='utf-8')\n",
    "    else:\n",
    "        data_file_run2 = open(error_path+framework+\"/runtime_accuracy_{}2.csv\".format(model_short_name), mode='a+', newline='', encoding='utf-8')\n",
    "        flag = 1\n",
    "    data_writer_run2 = csv.writer(data_file_run2, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    if flag == 0: \n",
    "        data_writer_run2.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'test_set','execution_times','accuracy_original', 'accuracy_onnx', 'accuracy_coreml', 'miss_test_original', 'miss_original_onnx', 'miss_test_onnx', 'miss_original_coreml', 'miss_test_coreml','', 'inference_time_original', 'inference_time_onnx', 'inference_time_coreml', '', 'inference_time_original2', 'inference_time_onnx2', 'inference_time_coreml2', 'overral_time'])\n",
    "\n",
    "\n",
    "    for batch_size in [1]:\n",
    "        print(\"################ Batch size: \", batch_size, ' traiining id: ', training_id)\n",
    "        val_generator =  DataGenerator(x_test, y_test, batch_size=batch_size,  dim = input_shape,  n_classes= nb_classes, to_fit=True, shuffle=True)\n",
    "        _lets_convert(val_generator, data_writer_run2, batch_size)\n",
    "        #data_writer_acc.writerow([model_short_name,framework, training_id, model_name, batch_size, correct_original, correct_onnx, correct_coreml])\n",
    "    data_file_run2.close()\n",
    "#data_file_acc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator =  DataGenerator(x_test, y_test, batch_size=1,  dim = input_shape, \n",
    "                               n_classes= nb_classes, \n",
    "                               to_fit=True, shuffle=True)\n",
    "for i, (images, labels) in enumerate(val_generator):\n",
    "    x, y = images, labels\n",
    "    break\n",
    "k_predict = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "size2 = os.path.getsize(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "#def to_numpy(tensor):\n",
    "#    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "since_1 = time.time()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arrays are not equal\n",
      "\n",
      "Mismatched elements: 5 / 10 (50%)\n",
      "Max absolute difference: 1.6805134e-18\n",
      "Max relative difference: 3.799236e-06\n",
      " x: array([[2.118650e-21, 5.607732e-18, 1.213525e-19, 5.153317e-23,\n",
      "        1.000000e+00, 5.534655e-22, 2.125291e-18, 1.984877e-18,\n",
      "        2.307871e-17, 8.741971e-13]], dtype=float32)\n",
      " y: array([[2.118650e-21, 5.607732e-18, 1.213525e-19, 5.153297e-23,\n",
      "        1.000000e+00, 5.534634e-22, 2.125290e-18, 1.984877e-18,\n",
      "        2.307863e-17, 8.741955e-13]], dtype=float32)\n",
      "5 / 10 (50%) 50%\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    np.testing.assert_array_equal(k_predict, ort_outs[0])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    for line_ in str(e).split('\\n'):\n",
    "        #print(' ---- : ', line_)\n",
    "        if 'Mismatched elements' in line_:\n",
    "            value = line_.replace('Mismatched elements: ', '').strip()\n",
    "            perc_val = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "            print(value, perc_val)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = to_categorical(np.argmax(k_predict, 1), num_classes = 10)\n",
    "y1 = to_categorical(np.argmax(ort_outs[0], 1), num_classes = 10)\n",
    "#y2 = to_categorical(np.argmax(k_predict, 1), num_classes = 10)\n",
    "try:\n",
    "    np.testing.assert_array_equal(y, y0)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    for line_ in str(e).split('\\n'):\n",
    "        #print(' ---- : ', line_)\n",
    "        if 'Mismatched elements' in line_:\n",
    "            value = line_.replace('Mismatched elements: ', '').strip()\n",
    "            perc_val = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "            print(value, perc_val)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7533409e-02, 1.7899086e-01, 6.0070749e-02, ..., 7.2314382e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070749e-02, ..., 7.2314382e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070749e-02, ..., 7.2314382e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       ...,\n",
       "       [7.7532411e-02, 1.7899041e-01, 6.0070083e-02, ..., 7.2314888e-02,\n",
       "        1.4665152e-01, 7.3707536e-02],\n",
       "       [7.7603646e-02, 1.7887358e-01, 6.0151797e-02, ..., 7.2238758e-02,\n",
       "        1.4651124e-01, 7.3571399e-02],\n",
       "       [1.0000000e+00, 3.0505841e-16, 4.4258446e-15, ..., 7.6639033e-16,\n",
       "        5.0008275e-16, 3.2518154e-17]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y2 = to_categorical(np.argmax(ort_outs[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x.flatten()\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = coreml_model.get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_anchors = 1\n",
    "spec.description.output[0].type.multiArrayType.shape.append(num_classes)\n",
    "spec.description.output[0].type.multiArrayType.shape.append(num_anchors)\n",
    "#del spec.description.output[1].type.multiArrayType.shape[-1]\n",
    "#spec = coremltools.utils.convert_neural_network_spec_weights_to_fp16(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input {\n",
       "  name: \"conv2d_input\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      shape: 1\n",
       "      shape: 28\n",
       "      shape: 28\n",
       "      shape: 1\n",
       "      dataType: FLOAT32\n",
       "      shapeRange {\n",
       "        sizeRanges {\n",
       "          lowerBound: 1\n",
       "          upperBound: -1\n",
       "        }\n",
       "        sizeRanges {\n",
       "          lowerBound: 28\n",
       "          upperBound: 28\n",
       "        }\n",
       "        sizeRanges {\n",
       "          lowerBound: 28\n",
       "          upperBound: 28\n",
       "        }\n",
       "        sizeRanges {\n",
       "          lowerBound: 1\n",
       "          upperBound: 1\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "output {\n",
       "  name: \"Identity\"\n",
       "  type {\n",
       "    multiArrayType {\n",
       "      dataType: FLOAT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "metadata {\n",
       "  userDefined {\n",
       "    key: \"com.github.apple.coremltools.source\"\n",
       "    value: \"tensorflow==2.5.0\"\n",
       "  }\n",
       "  userDefined {\n",
       "    key: \"com.github.apple.coremltools.version\"\n",
       "    value: \"5.0\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"conv2d_input\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 1\n",
      "    shape: 28\n",
      "    shape: 28\n",
      "    shape: 1\n",
      "    dataType: FLOAT32\n",
      "    shapeRange {\n",
      "      sizeRanges {\n",
      "        lowerBound: 1\n",
      "        upperBound: -1\n",
      "      }\n",
      "      sizeRanges {\n",
      "        lowerBound: 28\n",
      "        upperBound: 28\n",
      "      }\n",
      "      sizeRanges {\n",
      "        lowerBound: 28\n",
      "        upperBound: 28\n",
      "      }\n",
      "      sizeRanges {\n",
      "        lowerBound: 1\n",
      "        upperBound: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Print input description to get input shape.\n",
    "print(coreml_model.get_spec().description.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input {\n",
      "  name: \"conv2d_input\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      shape: 1\n",
      "      shape: 28\n",
      "      shape: 28\n",
      "      shape: 1\n",
      "      dataType: FLOAT32\n",
      "      shapeRange {\n",
      "        sizeRanges {\n",
      "          lowerBound: 1\n",
      "          upperBound: -1\n",
      "        }\n",
      "        sizeRanges {\n",
      "          lowerBound: 28\n",
      "          upperBound: 28\n",
      "        }\n",
      "        sizeRanges {\n",
      "          lowerBound: 28\n",
      "          upperBound: 28\n",
      "        }\n",
      "        sizeRanges {\n",
      "          lowerBound: 1\n",
      "          upperBound: 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"Identity\"\n",
      "  type {\n",
      "    multiArrayType {\n",
      "      dataType: FLOAT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata {\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.source\"\n",
      "    value: \"tensorflow==2.5.0\"\n",
      "  }\n",
      "  userDefined {\n",
      "    key: \"com.github.apple.coremltools.version\"\n",
      "    value: \"5.0\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(coreml_model.get_spec().description) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name), compute_units=coremltools.ComputeUnit.CPU_ONLY)\n",
    "\n",
    "output_dict_test = coreml_model.predict({'conv2d_input':x})\n",
    "####### Mis-classification coreML ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd_model = coremltools.models.MLModel(spec)\n",
    "output_dict_test2 = ssd_model.predict({'conv2d_input':x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3_ = to_categorical(np.argmax(output_dict_test2['Identity'], 1))\n",
    "output_dict_test2['Identity'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict_test['Identity'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = to_categorical(np.argmax(output_dict_test['Identity'], 1), num_classes = 10)\n",
    "y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-50d63a203713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Identity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.statsmodels'"
     ]
    }
   ],
   "source": [
    "from sklearn.statsmodels.tools import categorical\n",
    "b = categorical(output_dict_test['Identity'], drop=True)\n",
    "b.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       ...,\n",
       "       [7.7532411e-02, 1.7899041e-01, 6.0070083e-02, ..., 7.2314888e-02,\n",
       "        1.4665152e-01, 7.3707536e-02],\n",
       "       [7.7603646e-02, 1.7887357e-01, 6.0151797e-02, ..., 7.2238743e-02,\n",
       "        1.4651124e-01, 7.3571399e-02],\n",
       "       [1.0000000e+00, 3.0505955e-16, 4.4258611e-15, ..., 7.6639615e-16,\n",
       "        5.0008465e-16, 3.2518280e-17]], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       [7.7533409e-02, 1.7899086e-01, 6.0070757e-02, ..., 7.2314389e-02,\n",
       "        1.4665432e-01, 7.3706582e-02],\n",
       "       ...,\n",
       "       [7.7532418e-02, 1.7899041e-01, 6.0070086e-02, ..., 7.2314896e-02,\n",
       "        1.4665154e-01, 7.3707543e-02],\n",
       "       [7.7603646e-02, 1.7887357e-01, 6.0151801e-02, ..., 7.2238743e-02,\n",
       "        1.4651124e-01, 7.3571399e-02],\n",
       "       [1.0000000e+00, 3.0505841e-16, 4.4258446e-15, ..., 7.6639028e-16,\n",
       "        5.0008465e-16, 3.2518154e-17]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict_test['Identity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = to_categorical(np.argmax(output_dict_test['Identity'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 5)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y3)):\n",
    "    print(y3[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\n(shapes (128, 10), (128, 9) mismatch)\n x: array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],...\n y: array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-72096f1c3a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_equal\u001b[0;34m(x, y, err_msg, verbose)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Hide traceback for py.test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n\u001b[0;32m--> 931\u001b[0;31m                          verbose=verbose, header='Arrays are not equal')\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    757\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mflagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not equal\n\n(shapes (128, 10), (128, 9) mismatch)\n x: array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],...\n y: array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],..."
     ]
    }
   ],
   "source": [
    "np.testing.assert_array_equal(y, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.testing.assert_array_equal(y, y3)\n",
    "except Exception as e:\n",
    "    #print(e)\n",
    "    for line_ in str(e).split('\\n'):\n",
    "        #print(' ---- : ', line_)\n",
    "        if 'Mismatched elements' in line_:\n",
    "            value = line_.replace('Mismatched elements: ', '').strip()\n",
    "            perc_val = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "            print(value, perc_val)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
