{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( '/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "17866    [there, vogue, past, years, often, ironic, zom...\n",
      "31472    [three, giant, sabretooth, tigers, created, la...\n",
      "9636     [the, film, opens, director, talking, camera, ...\n",
      "28275    [being, independent, filmmaker, huge, fan, edw...\n",
      "42919    [i, went, movie, theater, afternoon, expecting...\n",
      "                               ...                        \n",
      "49328    [michael, ritchie, the, couch, trip, wonderful...\n",
      "833      [all, i, ever, heard, raised, equality, sexes,...\n",
      "5937     [i, never, expected, much, film, trashy, b, mo...\n",
      "19673    [there, never, dull, moment, movie, wonderful,...\n",
      "18382    [like, another, reviewer, wife, bought, movie,...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "28050    [will, smith, delivers, yet, film, man, weight...\n",
      "23513    [on, distant, planet, psychopath, saved, execu...\n",
      "17323    [i, give, less, star, i, tried, at, moment, im...\n",
      "32257    [there, seems, money, behind, film, would, imp...\n",
      "25660    [i, remember, seeing, movie, shown, several, y...\n",
      "                               ...                        \n",
      "47448    [i, normally, compelled, write, review, film, ...\n",
      "17899    [i, watching, every, night, vh, past, week, th...\n",
      "22613    [marjorie, splendid, riveting, performance, fa...\n",
      "28399    [an, average, tv, movie, quality, totally, for...\n",
      "1621     [quite, unimpressive, the, twists, pretty, pre...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "17866    0\n",
      "31472    0\n",
      "9636     1\n",
      "28275    0\n",
      "42919    1\n",
      "        ..\n",
      "49328    1\n",
      "833      0\n",
      "5937     0\n",
      "19673    1\n",
      "18382    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "28050    1\n",
      "23513    0\n",
      "17323    0\n",
      "32257    0\n",
      "25660    1\n",
      "        ..\n",
      "47448    1\n",
      "17899    1\n",
      "22613    1\n",
      "28399    0\n",
      "1621     0\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[   50 12879   422 ...     4 21832 39825]\n",
      " [  182  1216 12882 ...  1680  2957 11137]\n",
      " [    2     4  1982 ...     0     0     0]\n",
      " ...\n",
      " [    1    40   742 ...   365   102     3]\n",
      " [   50    40   667 ...     0     0     0]\n",
      " [    6    73  2103 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[ 1435  1244  1462 ...  1560     1  1299]\n",
      " [  433  3625  1140 ...   409   754   745]\n",
      " [    1   105   242 ...     0     0     0]\n",
      " ...\n",
      " [10938  4080  4281 ... 24973  9015  3908]\n",
      " [  715   739   136 ...     0     0     0]\n",
      " [   90  9514     2 ...     0     0     0]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = 10\n",
    "model_short_name = 'gru'\n",
    "framework = 'keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240310344"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "since_0 = time.time()\n",
    "#model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "model_name = 'tf_gru-imdb_2021-10-31_{}'.format(training_id)\n",
    "model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "t_elapsed_0 = time.time() - since_0\n",
    "size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "size0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.5.0 detected. Last version known to be fully compatible is 2.3.1 .\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import coremltools\n",
    "import time\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/onnx/'\n",
    "coreml_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/coremltools/'\n",
    "error_path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/errors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_category(y):\n",
    "    list_ = []\n",
    "    for i in y:\n",
    "        val = 1\n",
    "        if i < 0.5:\n",
    "            val = 0\n",
    "        list_.append(val)\n",
    "    return np.array(list_)\n",
    "#convert_category(k_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(y_test, test_predict):\n",
    "    correct_ = np.sum(y_test == test_predict)\n",
    "    accuracy  = correct_*100./np.sum(y_test == y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, y, data_writer_run, batch_size):\n",
    "    \n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "    print(\"converting for batch: \", i)\n",
    "    \n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    ### Original Model\n",
    "    since_1 = time.time()\n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    try:\n",
    "        with tf.device('/cpu:0'): \n",
    "            k_predict = model.predict(x)\n",
    "    except Exception as e:\n",
    "        print('Error keras: ', e)\n",
    "        return\n",
    "    inference_time_original = time.time() - since_1\n",
    "    y0 = convert_category(k_predict)\n",
    "    correct_original = np.sum(y0 == y)\n",
    "    accuracy_original = model_scores(y, y0)\n",
    "    # ONNX Model\n",
    "    \n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    since_1 = time.time()\n",
    "    onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    load_time_onnx = time.time() - since_1\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    inference_time_onnx = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    \n",
    "    ####### Mis-classification ONNX ######################################\n",
    "    y2 = convert_category(ort_outs[0])\n",
    "    correct_onnx = np.sum(y2 == y)\n",
    "    accuracy_onnx = model_scores(y, y2)\n",
    "    miss_perc_val_original_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(k_predict, ort_outs[0])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    encoded_miss_perc_val_original_onnx = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                encoded_miss_perc_val_original_onnx = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    miss_perc_val_test_runtime = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y2)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    ####### End of mis-classification ONNX ###################################### \n",
    "    \n",
    "    \n",
    "    ## CoreML\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    #spec = coreml_model.get_spec()\n",
    "    #coreml_model = coremltools.models.MLModel(spec)\n",
    "    split_ = str(coreml_model.get_spec().description.input[0]).split('\\n')\n",
    "    name_1 = split_[0].replace('name: \"', '')\n",
    "    name_1 = name_1.replace('\"', '')\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    output_dict_test = coreml_model.predict({name_1:x})\n",
    "    inference_time_coreml = time.time() - since_1\n",
    "    ####### Mis-classification coreML ######################################\n",
    "    y3 = convert_category(output_dict_test['Identity'])\n",
    "    correct_coreml = np.sum(y3 == y)\n",
    "    accuracy_coreml = model_scores(y, y3)\n",
    "    \n",
    "    #print(correct_original, correct_coreml, correct_onnx, np.sum(y == y))\n",
    "    ## Part 1\n",
    "    \n",
    "    miss_perc_val_original_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(k_predict, output_dict_test['Identity'])\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    ####### Part 2\n",
    "    #print('default-shape: ',k_predict.shape, 'onnx-shape: ',ort_outs[0].shape, 'coreml-shape: ',output_dict_test['Identity'].shape)\n",
    "    miss_perc_val_original_coreml = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y0, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_original_coreml = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    \n",
    "    miss_perc_val_test_runtime2 = 0\n",
    "    try:\n",
    "        np.testing.assert_array_equal(y, y3)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        for line_ in str(e).split('\\n'):\n",
    "            #print(' ---- : ', line_)\n",
    "            if 'Mismatched elements' in line_:\n",
    "                value = line_.replace('Mismatched elements: ', '').strip()\n",
    "                miss_perc_val_test_runtime2 = value[value.find(\"(\")+1:value.find(\")\")]\n",
    "                #print(value, perc_val)\n",
    "                break\n",
    "    ####### End of mis-classification coreML ######################################\n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, batch_size, i,'onnx',t_elapsed_0, inference_time_original, load_time_onnx, \n",
    "                          inference_time_onnx,  miss_perc_val_original_runtime,'',  encoded_miss_perc_val_original_onnx, miss_perc_val_test_runtime, '', accuracy_original, accuracy_onnx])\n",
    "    \n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, batch_size, i,'coremltools',t_elapsed_0, inference_time_original, load_time_coreml, \n",
    "                          inference_time_coreml,  miss_perc_val_original_runtime2,'',  miss_perc_val_original_coreml, miss_perc_val_test_runtime2, '', accuracy_original,accuracy_coreml])\n",
    "    \n",
    "    #return correct_original,correct_onnx,correct_coreml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def _lets_convert(data, data_writer_run, batch_size):\n",
    "    since = time.time()\n",
    "    for i, (inputs, labels) in enumerate(data):\n",
    "        inputs = to_numpy(inputs)\n",
    "        labels = to_numpy(labels)\n",
    "        to_onnx(i, inputs,labels, data_writer_run, batch_size)\n",
    "        if i == 50:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Batch size:  128\n",
      "converting for batch:  0\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  1\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  2\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  3\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  4\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  5\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  6\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  7\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  8\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  9\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  10\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  11\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  12\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  13\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  14\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  15\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  16\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  17\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  18\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  19\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  20\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  21\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  22\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  23\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  24\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  25\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  26\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  27\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  28\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  29\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  30\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  31\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  32\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  33\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  34\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  35\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  36\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  37\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  38\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  39\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  40\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  41\n",
      "Error keras:   indices[25,41] = 92499 is not in [0, 92492)\n",
      "\t [[node model_1/embedding_1/embedding_lookup (defined at <ipython-input-17-2ad63f1c5592>:17) ]] [Op:__inference_predict_function_3557]\n",
      "\n",
      "Errors may have originated from an input operation.\n",
      "Input Source operations connected to node model_1/embedding_1/embedding_lookup:\n",
      " model_1/embedding_1/embedding_lookup/2013 (defined at /opt/anaconda3/lib/python3.7/contextlib.py:112)\n",
      "\n",
      "Function call stack:\n",
      "predict_function\n",
      "\n",
      "converting for batch:  42\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  43\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  44\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  45\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  46\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  47\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  48\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  49\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "converting for batch:  50\n",
      "\n",
      "*********\n",
      "\n",
      "\n",
      "Conversion complete in 3m 42s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "if not os.path.exists(error_path+framework+\"/{}\".format(model_short_name)):\n",
    "        Path(error_path+framework+\"/{}\".format(model_short_name)).mkdir(parents=True, exist_ok=True)\n",
    "data_file_run = open(error_path+framework+\"/{}/runtime_miss-classification_{}.csv\".format(model_short_name,model_name), mode='w', newline='',\n",
    "                                  encoding='utf-8')\n",
    "data_writer_run = csv.writer(data_file_run, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "data_writer_run.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'round','runtime','original_load_time', 'original_infererence_time', 'runtime_load_time', \n",
    "                          'runtime_inference_time',  'miss_classified_original_runtime_percentage','',  'encoded_miss_classified_original_runtime_percentage','encoded_miss_classified_original_test_runtime_percentage', '', 'accuracy_original', 'accuracy_runtime'])\n",
    "\n",
    "for batch_size in [128]:\n",
    "    print(\"################ Batch size: \", batch_size)\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.as_tensor(np.array(x_train).astype('int32')), torch.as_tensor(np.array(y_train).astype('int32')))\n",
    "    valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "\n",
    "    # dataloaders\n",
    "    #batch_size = batch_size_\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    _lets_convert(valid_loader, data_writer_run, batch_size)\n",
    "    #data_writer_acc.writerow([model_short_name,framework, training_id, model_name, batch_size, correct_original, correct_onnx, correct_coreml])\n",
    "data_file_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(valid_loader):\n",
    "    inputs = to_numpy(inputs)\n",
    "    labels = to_numpy(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with tf.device('/cpu:0'): \n",
    "        k_predict = model.predict(inputs)\n",
    "except Exception as e:\n",
    "    print('Error keras: ', e)\n",
    "    #return\n",
    "y0 = convert_category(k_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00763112]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y00 = np.argmax(k_predict)\n",
    "y00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
