{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv( '/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('/Volumes/Cisco/Fall2021/onnx-exchange/Training/IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "27186    [the, movie, incredible, sound, track, sets, t...\n",
      "22304    [life, pod, one, movies, watch, try, analyze, ...\n",
      "7313     [the, film, artemisia, may, considered, treaso...\n",
      "17416    [well, hero, terror, slightly, average, opinio...\n",
      "4340     [i, think, scarecrows, creepy, pity, movie, ma...\n",
      "                               ...                        \n",
      "5620     [gi, samurai, sees, sonny, chiba, guys, get, t...\n",
      "48623    [i, huge, fan, big, loud, trashy, completely, ...\n",
      "44531    [two, great, comedians, great, neil, simon, mo...\n",
      "37186    [when, art, going, overcome, racism, i, believ...\n",
      "23344    [this, thought, provoking, capital, punishment...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "25953    [spoiler, warning, when, main, character, sist...\n",
      "33052    [i, planned, leaving, review, seeing, dreadful...\n",
      "38686    [the, first, five, st, trinian, films, althoug...\n",
      "7198     [i, went, movie, hoping, imaginative, twist, s...\n",
      "24264    [you, may, consider, couple, facts, discussion...\n",
      "                               ...                        \n",
      "44892    [like, lot, people, i, loved, original, americ...\n",
      "34417    [see, three, colors, blue, three, colors, whit...\n",
      "28357    [very, intelligent, humor, excellent, performi...\n",
      "7591     [if, ever, visited, shenandoah, acres, child, ...\n",
      "17459    [great, documentary, scientist, believed, dino...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "27186    1\n",
      "22304    1\n",
      "7313     1\n",
      "17416    0\n",
      "4340     0\n",
      "        ..\n",
      "5620     1\n",
      "48623    1\n",
      "44531    1\n",
      "37186    1\n",
      "23344    1\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "25953    0\n",
      "33052    1\n",
      "38686    1\n",
      "7198     0\n",
      "24264    0\n",
      "        ..\n",
      "44892    0\n",
      "34417    1\n",
      "28357    1\n",
      "7591     0\n",
      "17459    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[    2     3   988 ...     0     0     0]\n",
      " [   42  8807     5 ...     0     0     0]\n",
      " [    2     4 10626 ...     0     0     0]\n",
      " ...\n",
      " [   35    20  5086 ...     0     0     0]\n",
      " [  171   395    81 ...     0     0     0]\n",
      " [    9   104  2587 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[1235 1670  171 ...    0    0    0]\n",
      " [   1 3717 1116 ...    0    0    0]\n",
      " [   2   23  607 ...    0    0    0]\n",
      " ...\n",
      " [ 819  998  366 ...    0    0    0]\n",
      " [  55   51 5758 ...    0    0    0]\n",
      " [  20  543 1464 ...    0    0    0]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.2 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.5.0 detected. Last version known to be fully compatible is 2.3.1 .\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import coremltools\n",
    "import time\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/onnx/'\n",
    "coreml_path = '/Volumes/Cisco/Fall2021/onnx-exchange/conversion/coremltools/'\n",
    "error_path = '/Volumes/Cisco/Fall2021/onnx-exchange/miss-classification/v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_category(y):\n",
    "    \n",
    "    #list_ = []\n",
    "    #for i in y:\n",
    "    #    val = 1\n",
    "    #    if i < 0.5:\n",
    "    #        val = 0\n",
    "    #    list_.append(val)\n",
    "    return np.argmax(y) #np.array(list_)\n",
    "#convert_category(k_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(y_test, test_predict):\n",
    "    correct_ = 0 #y_test = test_predict #np.sum(y_test == test_predict)\n",
    "    if y_test == test_predict:\n",
    "        correct_ = 1\n",
    "    accuracy  = correct_#*100./np.sum(y_test == y_test)\n",
    "    return accuracy\n",
    "def get_miss_classification(y1, y2):\n",
    "    miss_perc_val_original_coreml = 0\n",
    "    if y1 != y2:\n",
    "        miss_perc_val_original_coreml = 1\n",
    "    return miss_perc_val_original_coreml #, precision2, recall2, f12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onnx(i, x, y, batch_size):\n",
    "    \n",
    "    # Input to the model\n",
    "    #device_reset = cuda.get_current_device()\n",
    "    #device_reset.reset()\n",
    "    #x.cuda()\n",
    "    #print(\"converting for batch: \", i)\n",
    "    y = np.argmax(y)\n",
    "    #torch.random.manual_seed(42)\n",
    "    #x = torch.randn(10000, 3, 32, 32, requires_grad=True)\n",
    "    \n",
    "    ### Original Model\n",
    "    since_1 = time.time()\n",
    "    #model = torch.load(path+model_name+'.pth')\n",
    "    try:\n",
    "        with tf.device('/cpu:0'): \n",
    "            k_predict = model.predict(x)\n",
    "    except Exception as e:\n",
    "        print('Error keras: ')\n",
    "        return None\n",
    "    inference_time_original = time.time() - since_1\n",
    "    y0 = convert_category(k_predict)\n",
    "    correct_original = np.sum(y0 == y)\n",
    "    accuracy_original = model_scores(y, y0)\n",
    "    miss_test_original = get_miss_classification(y, y0)\n",
    "    # ONNX Model\n",
    "    \n",
    "    t_elapsed_2 = time.time() - since_1\n",
    "    #since_1 = time.time()\n",
    "    #onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    #load_time_onnx = time.time() - since_1\n",
    "    #onnx.checker.check_model(onnx_model)\n",
    "    #def to_numpy(tensor):\n",
    "    #    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    #ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    since_1 = time.time()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    inference_time_onnx = time.time() - since_1\n",
    "    # compare ONNX Runtime and PyTorch results\n",
    "    #print(\"\\n*********\\n\\n\")\n",
    "    #time_diff = t_elapsed_0+t_elapsed_1, t_elapsed_2, t_elapsed_3\n",
    "    \n",
    "    ####### Mis-classification ONNX ######################################\n",
    "    y2 = convert_category(ort_outs[0])\n",
    "    correct_onnx = np.sum(y2 == y)\n",
    "    accuracy_onnx = model_scores(y, y2)\n",
    "    miss_original_onnx = get_miss_classification(y0, y2)\n",
    "    miss_test_onnx = get_miss_classification(y, y2)\n",
    "    ####### End of mis-classification ONNX ###################################### \n",
    "    \n",
    "    \n",
    "    ## CoreML\n",
    "    \n",
    "    #since_1 = time.time()\n",
    "    #coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    #load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    #spec = coreml_model.get_spec()\n",
    "    #coreml_model = coremltools.models.MLModel(spec)\n",
    "    split_ = str(coreml_model.get_spec().description.input[0]).split('\\n')\n",
    "    name_1 = split_[0].replace('name: \"', '')\n",
    "    name_1 = name_1.replace('\"', '')\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    output_dict_test = coreml_model.predict({name_1:x})\n",
    "    inference_time_coreml = time.time() - since_1\n",
    "    ####### Mis-classification coreML ######################################\n",
    "    y3 = convert_category(output_dict_test['Identity'])\n",
    "    correct_coreml = np.sum(y3 == y)\n",
    "    accuracy_coreml = model_scores(y, y3)\n",
    "    miss_original_coreml = get_miss_classification(y0, y3)\n",
    "    miss_test_coreml = get_miss_classification(y, y3)\n",
    "    \n",
    "    ####### End of mis-classification coreML ######################################\n",
    "    #return correct_original,correct_onnx,correct_coreml \n",
    "    list_val = [accuracy_original, accuracy_onnx, accuracy_coreml, miss_original_onnx, miss_original_coreml,inference_time_original, inference_time_onnx, inference_time_coreml]\n",
    "    return list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def _lets_convert(data, data_writer_run, batch_size):\n",
    "    since = time.time()\n",
    "    accuracy_original = 0.0\n",
    "    accuracy_onnx = 0.0\n",
    "    accuracy_coreml = 0.0\n",
    "    #miss_test_original = 0\n",
    "    miss_original_onnx = 0\n",
    "    #miss_test_onnx = 0\n",
    "    miss_original_coreml = 0\n",
    "    #miss_test_coreml = 0\n",
    "    \n",
    "    inference_time_original = 0.0\n",
    "    inference_time_onnx = 0.0\n",
    "    inference_time_coreml = 0.0\n",
    "    \n",
    "    total_ = 0.0\n",
    "    total_datasets = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(data):\n",
    "        inputs = to_numpy(inputs)\n",
    "        labels = to_numpy(labels)\n",
    "        list_val = to_onnx(i, inputs,labels, batch_size)\n",
    "        if i%50 == 0:\n",
    "            print(i, list_val)\n",
    "        if list_val != None:\n",
    "            accuracy_original += list_val[0]\n",
    "            accuracy_onnx += list_val[1]\n",
    "            accuracy_coreml += list_val[2]\n",
    "\n",
    "            miss_original_onnx += list_val[3]\n",
    "            miss_original_coreml += list_val[4]\n",
    "\n",
    "            inference_time_original += list_val[5]\n",
    "            inference_time_onnx += list_val[6]\n",
    "            inference_time_coreml += list_val[7]\n",
    "            total_ += np.sum(labels == labels)\n",
    "            total_datasets += labels.shape[0]\n",
    "        if i == 1000:\n",
    "            break\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Conversion complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60) )\n",
    "    #print(correct_original, correct_onnx, correct_coreml, total, correct_original*100./total, precision2, recall2, f1)\n",
    "    #return correct_original*100./total, correct_onnx*100./total, correct_coreml*100./total\n",
    "    data_writer_run.writerow([model_short_name,framework, training_id, model_name, round(accuracy_original*100/total_,2), round(accuracy_onnx*100/total_,2), round(accuracy_coreml*100/total_,2),  round(miss_original_onnx*100/total_datasets,2),  round(miss_original_coreml*100/total_datasets,2),'', inference_time_original/total_datasets, inference_time_onnx/total_datasets, inference_time_coreml/total_datasets, '',\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_original/total_datasets) // 60, (inference_time_original/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_onnx/total_datasets) // 60, (inference_time_onnx/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format((inference_time_coreml/total_datasets) // 60, (inference_time_coreml/total_datasets) % 60),\n",
    "                              '{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:  1\n",
      "0 [1, 1, 1, 0, 0, 2.839478015899658, 0.040728092193603516, 0.06530189514160156]\n",
      "Error keras: \n",
      "50 [1, 1, 1, 0, 0, 0.0899651050567627, 0.04254484176635742, 0.03223395347595215]\n",
      "100 [1, 1, 1, 0, 0, 0.14689183235168457, 0.07509493827819824, 0.042842864990234375]\n",
      "150 [1, 1, 1, 0, 0, 0.12276816368103027, 0.05427908897399902, 0.041950225830078125]\n",
      "200 [1, 1, 1, 0, 0, 0.10401511192321777, 0.04390764236450195, 0.03137683868408203]\n",
      "250 [1, 1, 1, 0, 0, 0.08327126502990723, 0.03220772743225098, 0.03266024589538574]\n",
      "300 [1, 1, 1, 0, 0, 0.08024096488952637, 0.03349900245666504, 0.028935909271240234]\n",
      "350 [1, 1, 1, 0, 0, 0.08747696876525879, 0.03776812553405762, 0.031179189682006836]\n",
      "Error keras: \n",
      "400 [1, 1, 1, 0, 0, 0.09198212623596191, 0.03947281837463379, 0.0382540225982666]\n",
      "450 [1, 1, 1, 0, 0, 0.10223793983459473, 0.04369997978210449, 0.03444623947143555]\n",
      "500 [1, 1, 1, 0, 0, 0.07384586334228516, 0.03658103942871094, 0.024842023849487305]\n",
      "Error keras: \n",
      "550 [1, 1, 1, 0, 0, 0.0615696907043457, 0.027544260025024414, 0.02285599708557129]\n",
      "600 [1, 1, 1, 0, 0, 0.1793069839477539, 0.034101009368896484, 0.02634906768798828]\n",
      "Error keras: \n",
      "650 [1, 1, 1, 0, 0, 1.1441259384155273, 0.5477690696716309, 0.41410088539123535]\n",
      "700 [1, 1, 1, 0, 0, 0.10535287857055664, 0.06249666213989258, 0.03662300109863281]\n",
      "750 [1, 1, 1, 0, 0, 0.4415431022644043, 0.10110282897949219, 0.15662384033203125]\n",
      "800 [1, 1, 1, 0, 0, 0.3121070861816406, 0.08837771415710449, 0.039354801177978516]\n",
      "850 [1, 1, 1, 0, 0, 0.1595151424407959, 0.12332010269165039, 0.046148061752319336]\n",
      "900 [1, 1, 1, 0, 0, 0.3050670623779297, 0.0633399486541748, 0.08501100540161133]\n",
      "950 [1, 1, 1, 0, 0, 0.7393910884857178, 0.5622589588165283, 0.0930020809173584]\n",
      "1000 [1, 1, 1, 0, 0, 0.11397099494934082, 0.039192914962768555, 0.03691530227661133]\n",
      "Conversion complete in 30m 57s\n",
      "round:  2\n",
      "Error keras: \n",
      "0 None\n",
      "50 [1, 1, 1, 0, 0, 0.13832616806030273, 0.04773306846618652, 0.0373842716217041]\n",
      "100 [1, 1, 1, 0, 0, 0.3092648983001709, 0.1012270450592041, 0.0454869270324707]\n",
      "150 [1, 1, 1, 0, 0, 0.19781494140625, 0.06961202621459961, 0.04853487014770508]\n",
      "200 [1, 1, 1, 0, 0, 0.10508584976196289, 0.04051089286804199, 0.024837017059326172]\n",
      "250 [1, 1, 1, 0, 0, 0.12590479850769043, 0.07438516616821289, 0.03642010688781738]\n",
      "300 [1, 1, 1, 0, 0, 0.27872705459594727, 0.26991891860961914, 0.07948017120361328]\n",
      "350 [1, 1, 1, 0, 0, 0.9260389804840088, 0.40644216537475586, 0.07560896873474121]\n",
      "400 [1, 1, 1, 0, 0, 0.1724240779876709, 0.1647641658782959, 0.04754519462585449]\n",
      "Error keras: \n",
      "450 [1, 1, 1, 0, 0, 0.10849928855895996, 0.05289483070373535, 0.03547477722167969]\n",
      "500 [1, 1, 1, 0, 0, 0.12720489501953125, 0.058351993560791016, 0.03431224822998047]\n",
      "Error keras: \n",
      "550 [1, 1, 1, 0, 0, 0.12554597854614258, 0.06463479995727539, 0.034516096115112305]\n",
      "600 [1, 1, 1, 0, 0, 0.24699902534484863, 0.10726284980773926, 0.0500178337097168]\n",
      "650 [1, 1, 1, 0, 0, 0.1822659969329834, 0.05704498291015625, 0.0368192195892334]\n",
      "700 [1, 1, 1, 0, 0, 0.14855194091796875, 0.048651695251464844, 0.05398702621459961]\n",
      "750 [1, 1, 1, 0, 0, 0.08982396125793457, 0.04388785362243652, 0.0305941104888916]\n",
      "800 [1, 1, 1, 0, 0, 0.09976410865783691, 0.05231213569641113, 0.04121088981628418]\n",
      "850 [1, 1, 1, 0, 0, 0.11252188682556152, 0.04549980163574219, 0.029435157775878906]\n",
      "900 [1, 1, 1, 0, 0, 0.16283917427062988, 0.07070517539978027, 0.05893087387084961]\n",
      "950 [1, 1, 1, 0, 0, 0.17403006553649902, 0.19765782356262207, 0.06546878814697266]\n",
      "1000 [1, 1, 1, 0, 0, 0.3606998920440674, 0.2527289390563965, 0.04083108901977539]\n",
      "Conversion complete in 42m 4s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#data_file_run = open(error_path+framework+\"/{}/runtime_miss-classification_{}.csv\".format(model_short_name,model_name), mode='w', newline='',\n",
    "#                                  encoding='utf-8')\n",
    "#data_writer_run = csv.writer(data_file_run, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#data_writer_run.writerow(['model','framework', 'training_id', 'model_full', \"batch_size\", 'round','runtime','original_load_time', 'original_infererence_time', 'runtime_load_time', \n",
    "#                          'runtime_inference_time',  'miss_classified_original_runtime_percentage','',  'encoded_miss_classified_original_runtime_percentage','encoded_miss_classified_original_test_runtime_percentage', '', 'accuracy_original', 'accuracy_runtime'])\n",
    "\n",
    "for round_ in [3,4,5,6,7,8,9,10]: #,3,4,5,6,7,8,9,10\n",
    "    training_id = round_\n",
    "    model_short_name = 'lstm'\n",
    "    framework = 'keras'\n",
    "    path = '/Volumes/Cisco/Fall2021/onnx-exchange/Training/{}/{}/'.format(framework, model_short_name)\n",
    "    since_0 = time.time()\n",
    "    #model_path = 'tf_Lenet5_mnist_2021-08-24-10:35:35'\n",
    "    #model_name = 'tf_alexnet_cifar10_2021-08-27-17:05:27'\n",
    "    #model_name = 'tf_resnet18-cifar10_2021-10-29_{}'.format(training_id)\n",
    "    model_name = 'tf_lstm-imdb_2021-10-30_{}'.format(training_id)\n",
    "    #model_name = 'tf_gru-imdb_2021-10-29_{}'.format(training_id)\n",
    "    model = tf.keras.models.load_model(path+ model_name+'.h5')\n",
    "    t_elapsed_0 = time.time() - since_0\n",
    "    size0 = os.path.getsize(path+ model_name+'.h5')\n",
    "    size0\n",
    "    \n",
    "    since_1 = time.time()\n",
    "    onnx_model = onnx.load(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    load_time_onnx = time.time() - since_1\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path+framework+\"/{}/{}.onnx\".format(model_short_name, model_name))\n",
    "    \n",
    "    \n",
    "    since_1 = time.time()\n",
    "    coreml_model = coremltools.models.MLModel(coreml_path+framework+\"/{}/{}.mlmodel\".format(model_short_name, model_name))\n",
    "    load_time_coreml = time.time() - since_1\n",
    "    \n",
    "    if not os.path.exists(error_path+framework):\n",
    "        Path(error_path+framework).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print('round: ', round_)\n",
    "    flag = 0\n",
    "    if not os.path.exists(error_path+framework+\"/runtime_accuracy_{}.csv\".format(model_short_name)):\n",
    "        data_file_run2 = open(error_path+framework+\"/runtime_accuracy_{}.csv\".format(model_short_name), mode='w', newline='', encoding='utf-8')\n",
    "    else:\n",
    "        data_file_run2 = open(error_path+framework+\"/runtime_accuracy_{}.csv\".format(model_short_name), mode='a+', newline='', encoding='utf-8')\n",
    "        flag = 1\n",
    "    data_writer_run2 = csv.writer(data_file_run2, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    if flag == 0: \n",
    "        data_writer_run2.writerow(['model','framework', 'training_id', 'model_full', 'accuracy_original', 'accuracy_onnx', 'accuracy_coreml', 'miss_original_onnx', 'miss_original_coreml','', 'inference_time_original', 'inference_time_onnx', 'inference_time_coreml', '', 'inference_time_original2', 'inference_time_onnx2', 'inference_time_coreml2', 'overral_time'])\n",
    "        \n",
    "    #train_data = TensorDataset(torch.as_tensor(np.array(x_train).astype('int32')), torch.as_tensor(np.array(y_train).astype('int32')))\n",
    "    valid_data = TensorDataset(torch.as_tensor(np.array(x_test).astype('int32')), torch.as_tensor(np.array(y_test).astype('int32')))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 1 \n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    #train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    _lets_convert(valid_loader, data_writer_run2, batch_size)\n",
    "    data_file_run2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
